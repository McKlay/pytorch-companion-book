---
hide:
    - toc
---

# Chapter 3: Tensor Fundamentals

> â€œBefore we reshape the world with tensors, letâ€™s understand what they truly are.â€

---

## ðŸ” 3.1 What Is a Tensor?

A tensor is the core data structure in PyTorch. Itâ€™s a multi-dimensional array that represents data â€” and supports a wide range of mathematical operations on CPUs or GPUs.

But that definition alone is too dry. Letâ€™s break it down by dimensions:

| Tensor Dim | Common Name       | Example                  | Shape    |
|------------|-------------------|--------------------------|----------|
| 0D         | Scalar            | `42`                     | `()`     |
| 1D         | Vector            | `[1, 2, 3]`              | `(3,)`   |
| 2D         | Matrix            | `[[1, 2], [3, 4]]`       | `(2, 2)` |
| 3D+        | Tensor (generic)  | Batch of images, video   | `(B, C, H, W)` |

In PyTorch:

```python
import torch
x = torch.tensor([[1, 2], [3, 4]])
```
This creates a 2Ã—2 matrix â€” a rank-2 tensor.
All machine learning models, no matter how complex, are just functions that operate on tensors.

## 3.2 Tensors vs NumPy Arrays

|Feature	                |NumPy`ndarray`	                  |PyTorch`Tensor`      |
|---------------------------|---------------------------------|---------------------|
|GPU support	            |âŒ CPU only	                    |âœ… CUDA, MPS, etc.   |
|Autograd	                |âŒ No automatic gradients	    |âœ… Built-in autograd |
|Deep learning ready	    |âŒ Needs integration	        |âœ… Native support    |
|API similarity	            |âœ… High	                        |âœ… Nearly identical  |
|Interoperability	        |âœ… torch.from_numpy() / .numpy()|	                   |
|                           |                                 |                     | 

PyTorch was designed to mimic NumPyâ€™s API â€” so if youâ€™ve written code in NumPy before, tensors will feel familiar. But they also bring GPU acceleration and automatic differentiation into the mix.
```python
import numpy as np
np_arr = np.array([1.0, 2.0, 3.0])
torch_tensor = torch.from_numpy(np_arr)
# Back to NumPy
np_back = torch_tensor.numpy()
```
> âš ï¸ **Note:** Both share the same memory buffer unless you .clone() the tensor.

## 3.3 Device Management: CPU and CUDA

By default, PyTorch tensors live on the CPU. But the magic happens when you move them to the GPU (via CUDA).
**Check if CUDA is available:**
```python
torch.cuda.is_available()  # Returns True if you have a compatible GPU
```
**Move tensor to CUDA:**
```python
x = torch.tensor([1.0, 2.0])
x = x.to('cuda')        # Send to GPU
x = x.to('cpu')         # Bring back to CPU
```
You can also be explicit:
```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
x = torch.tensor([1.0, 2.0], device=device)
```

## 3.4 CPU vs GPU: When It Matters

|Use Case	                |CPU	                      |GPU                  |
|---------------------------|-----------------------------|---------------------|
|Small models or inference	|âœ… Fine	                    |âœ… Maybe overkill     |
|Deep neural nets	        |âŒ Too slow	                |âœ… Ideal               |
|Training loops w/ backprop	|âŒ Bottleneck	            |âœ… Accelerated         |
|Parallel data ops	        |âœ… w/ multiprocessing	    |âœ… Massively parallel  |
|                           |                            |                         |

> ðŸ’¡ Even simple tensor math is often 10â€“100Ã— faster on the GPU.

## 3.5 Summary

- A tensor is a generalization of scalars, vectors, and matrices.

- PyTorchâ€™s Tensor is like NumPyâ€™s ndarray, but with GPU and autograd support.

- Devices (cpu or cuda) matter â€” especially for training speed.

- Tensor creation is simple â€” but choosing the right device is crucial for performance.