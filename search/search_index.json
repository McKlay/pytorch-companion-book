{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Table of Contents","text":""},{"location":"#pytorch-builders-companion-book","title":"PyTorch Builder\u2019s Companion Book","text":""},{"location":"#a-deep-dive-into-the-torch-api-for-engineers-builders-and-curious-minds","title":"A Deep Dive into the Torch API for Engineers, Builders, and Curious Minds","text":""},{"location":"#contents","title":"Contents","text":""},{"location":"#preface","title":"\ud83d\udcd6 Preface","text":"<ul> <li>Why This Book Exists </li> <li>Who Should Read This </li> <li>From Tensors to Gradients: How This Book Was Born </li> <li>What You\u2019ll Learn (and What You Won\u2019t) </li> <li>How to Read This Book (Even if You\u2019re Just Starting Out)</li> </ul>"},{"location":"#part-i-getting-started-with-torch","title":"Part I \u2013 Getting Started with torch","text":"<p>\u00a0\u00a0\u00a0\u00a0 Chapter 1: What is torch? \u00a0\u00a0\u00a0\u00a0 Chapter 2: Installation &amp; Setup \u00a0\u00a0\u00a0\u00a0 Chapter 3: Tensor Fundamentals</p>"},{"location":"#part-ii-torch-api-deep-dive","title":"Part II \u2013 torch API Deep Dive","text":"<p>\u00a0\u00a0\u00a0\u00a0 Chapter 4: torch.Tensor \u00a0\u00a0\u00a0\u00a0 Chapter 5: Data Types and Devices \u00a0\u00a0\u00a0\u00a0 Chapter 6: Random Sampling and Seeding \u00a0\u00a0\u00a0\u00a0 Chapter 7: Math Operations \u00a0\u00a0\u00a0\u00a0 Chapter 8: Broadcasting and Shape Ops \u00a0\u00a0\u00a0\u00a0 Chapter 9: Autograd and Differentiation \u00a0\u00a0\u00a0\u00a0 Chapter 10: Type Conversions and Casting</p>"},{"location":"#part-iii-specialized-modules-in-torch","title":"Part III \u2013 Specialized Modules in torch","text":"<p>\u00a0\u00a0\u00a0\u00a0 Chapter 11: torch.linalg \u00a0\u00a0\u00a0\u00a0 Chapter 12: torch.nn.functional \u00a0\u00a0\u00a0\u00a0 Chapter 13: torch.special \u00a0\u00a0\u00a0\u00a0 Chapter 14: torch.fft \u00a0\u00a0\u00a0\u00a0 Chapter 15: torch.utils \u00a0\u00a0\u00a0\u00a0 Chapter 16: Storage and Memory Format</p>"},{"location":"#part-iv-torch-in-the-real-world","title":"Part IV \u2013 torch in the Real World","text":"<p>\u00a0\u00a0\u00a0\u00a0 Chapter 17: Using torch with CUDA \u00a0\u00a0\u00a0\u00a0 Chapter 18: Integration with NumPy \u00a0\u00a0\u00a0\u00a0 Chapter 19: Debugging, Profiling, and Best Practices</p>"},{"location":"#appendices","title":"\ud83d\udcce Appendices","text":"<p>\u00a0\u00a0\u00a0\u00a0 A. Tensor Shapes Cheat Sheet \u00a0\u00a0\u00a0\u00a0 B. PyTorch Idioms &amp; Gotchas \u00a0\u00a0\u00a0\u00a0 C. Full API Reference</p>"},{"location":"PartIII_overview/","title":"Part III: Specialized Modules in <code>torch</code>","text":"<p>This part of the book takes you beyond tensors and core math \u2014 into the specialized submodules of <code>torch</code> that unlock powerful capabilities for scientific computing, custom modeling, signal processing, and training workflows.</p> <p>Whether you're building large neural nets, performing linear algebra, or squeezing performance out of low-level memory layout \u2014 this section equips you with the tools.</p>"},{"location":"PartIII_overview/#chapter-11-torchlinalg","title":"Chapter 11: <code>torch.linalg</code>","text":"<p>This module brings modern, NumPy-style linear algebra into PyTorch:</p> <ul> <li>Matrix inversion: <code>torch.linalg.inv()</code> (use <code>solve()</code> instead when possible)</li> <li>Solving systems: <code>torch.linalg.solve(A, b)</code> for Ax = b</li> <li>Determinants, ranks, norms, and condition numbers</li> <li>Eigenvalues and SVD: <code>eig</code>, <code>svd</code>, <code>qr</code>, <code>lu</code></li> <li>GPU support, autograd compatibility, and batched ops</li> <li>Ideal for PCA, model diagnostics, and physics-inspired ML</li> </ul> <p>Pro tip: Prefer <code>solve()</code> over <code>inv()</code> for better numerical stability.</p>"},{"location":"PartIII_overview/#chapter-12-torchnnfunctional","title":"Chapter 12: <code>torch.nn.functional</code>","text":"<p>Stateless functions for deep learning components:</p> <ul> <li>Activations: <code>F.relu</code>, <code>F.sigmoid</code>, <code>F.softmax</code>, etc.</li> <li>Losses: <code>F.cross_entropy</code>, <code>F.mse_loss</code>, <code>F.binary_cross_entropy</code></li> <li>Functional layers: <code>F.linear</code>, <code>F.conv2d</code>, <code>F.pad</code>, <code>F.interpolate</code></li> <li>Use inside <code>forward()</code> methods for explicit control</li> <li>Essential for custom layers, dynamic architectures, and meta-learning</li> </ul> <p>Reminder: Never pass softmaxed logits into <code>F.cross_entropy()</code> \u2014 it expects raw logits.</p>"},{"location":"PartIII_overview/#chapter-13-torchspecial","title":"Chapter 13: <code>torch.special</code>","text":"<p>Advanced mathematical functions for specialized models:</p> <ul> <li><code>expit</code>, <code>erf</code>, <code>gamma</code>, <code>lgamma</code>, <code>digamma</code>, <code>xlogy</code>, <code>i0</code>, etc.</li> <li>Useful in: statistical distributions, variational inference, RL, and probabilistic modeling</li> <li>Inspired by SciPy's <code>scipy.special</code>, but supports autograd and GPU</li> <li>Handles numerical stability (e.g., <code>xlogy(0, 0)</code> safely)</li> </ul> <p>Use case: KL divergence, entropy, Bayesian models, and scientific ML.</p>"},{"location":"PartIII_overview/#chapter-14-torchfft","title":"Chapter 14: <code>torch.fft</code>","text":"<p>Time-to-frequency domain transformations:</p> <ul> <li>1D FFT: <code>fft</code>, <code>ifft</code>, <code>rfft</code>, <code>irfft</code></li> <li>2D/ND FFT: <code>fft2</code>, <code>ifft2</code>, <code>fftn</code></li> <li>Real-world use: audio signal analysis, image filtering, denoising, spectral CNNs</li> <li>All FFT outputs are complex tensors: <code>.real</code>, <code>.imag</code>, <code>.abs()</code>, <code>.angle()</code></li> </ul> <p>Example: Low-pass filtering noisy signals by zeroing high frequencies in FFT space.</p>"},{"location":"PartIII_overview/#chapter-15-torchutils","title":"\ud83d\udee0 Chapter 15: <code>torch.utils</code>","text":"<p>Utilities that make PyTorch practical and scalable:</p> <ul> <li><code>torch.utils.data</code>: Datasets and DataLoaders for efficient training</li> <li><code>tensorboard</code>: Visualize loss curves, histograms, metrics</li> <li><code>checkpoint</code>: Save memory by recomputing during backprop (useful in ResNets, Transformers)</li> <li>Others: <code>ConcatDataset</code>, <code>Subset</code>, <code>Sampler</code>, <code>cpp_extension</code>, <code>throughput_benchmark</code></li> </ul> <p>\ud83d\udca1 Reminder: You\u2019ll use <code>DataLoader</code> and <code>SummaryWriter</code> in almost every real-world project.</p>"},{"location":"PartIII_overview/#chapter-16-low-level-tensor-memory-storage","title":"Chapter 16: Low-Level Tensor Memory &amp; Storage","text":"<p>Understand the guts of how tensors are stored and accessed:</p> <ul> <li><code>.storage()</code>: Access the underlying flat memory</li> <li><code>.is_contiguous()</code>: Check for sequential memory layout</li> <li><code>.contiguous()</code>: Required for <code>.view()</code> and some backends</li> <li><code>memory_format</code>: Control layout for CNNs (NCHW vs NHWC)</li> <li><code>torch.save()</code> / <code>torch.load()</code> for raw tensor serialization</li> </ul> <p>Tip: Use <code>channels_last</code> layout for faster 2D convolutions on modern GPUs.</p>"},{"location":"PartIII_overview/#summary-of-part-iii","title":"Summary of Part III","text":"Chapter Submodule What You Gain 11 <code>torch.linalg</code> Modern linear algebra with GPU, autograd, and batching 12 <code>torch.nn.functional</code> Stateless, flexible layer &amp; loss functions 13 <code>torch.special</code> Advanced math for scientific, probabilistic, and stable modeling 14 <code>torch.fft</code> Frequency domain processing for signals and images 15 <code>torch.utils</code> Dataset loading, visualization, memory-saving, and training tools 16 <code>.storage</code>, <code>memory_format</code> Low-level control over memory, views, and performance tuning <p>This part unlocks the full computational power of PyTorch, from mathematics to model utilities. Whether you're building custom layers, optimizing convolutions, or working with scientific data \u2014 these tools are how you scale.</p> <p>\u2192 Next up: Part IV \u2014 Using CUDA, Mixed Precision, and Deployment Strategies.</p>"},{"location":"PartII_overview/","title":"Part II: <code>torch</code> API Deep Dive","text":"<p>Now that you've set up your environment and grasped the essence of <code>torch</code>, it\u2019s time to dive into its core APIs. This part is where torch transforms from a black box to a precision tool in your hands.</p> <p>From tensor manipulation to GPU control, autograd magic to precision casting \u2014 this section walks through everything you need to know to operate at the atomic level of PyTorch programming.</p>"},{"location":"PartII_overview/#chapter-4-torchtensor","title":"Chapter 4: <code>torch.Tensor</code>","text":"<p>The heart of PyTorch. This chapter covers:</p> <ul> <li>How to create tensors: from lists, with factory methods (<code>zeros</code>, <code>ones</code>, <code>full</code>, etc.), or based on other tensors</li> <li>Tensor attributes: <code>shape</code>, <code>dtype</code>, <code>device</code>, and <code>requires_grad</code></li> <li>Common operations: arithmetic, logical comparisons, reshaping, slicing</li> <li>Tensor reshaping: <code>view</code>, <code>reshape</code>, <code>permute</code>, <code>squeeze</code>, <code>unsqueeze</code></li> <li>Autograd compatibility: how tensors track operations to support backpropagation</li> <li>Pro tricks: <code>clone()</code>, <code>detach()</code>, and contiguous memory</li> </ul> <p>Understanding <code>torch.Tensor</code> is foundational \u2014 every model, every dataset, every operation begins here.</p>"},{"location":"PartII_overview/#chapter-5-data-types-and-devices","title":"Chapter 5: Data Types and Devices","text":"<p>Precision and location matter. This chapter explores:</p> <ul> <li><code>torch.dtype</code>: float32, float64, float16, int32, bool, and when to use each</li> <li><code>torch.device</code>: placing tensors on CPU vs GPU (and how to move them)</li> <li>Default dtype control: globally set default types for future tensors</li> <li>Mixed precision training: introduction to AMP for faster, memory-efficient training</li> </ul> <p>Mastering <code>dtype</code> and <code>device</code> ensures that your models run correctly, fast, and on the right hardware.</p>"},{"location":"PartII_overview/#chapter-6-random-sampling-and-seeding","title":"Chapter 6: Random Sampling and Seeding","text":"<p>Randomness is everywhere in ML \u2014 but chaos isn\u2019t always good. Learn how to:</p> <ul> <li>Generate random tensors with <code>rand</code>, <code>randn</code>, <code>randint</code>, <code>randperm</code>, and <code>bernoulli</code></li> <li>Set global seeds using <code>torch.manual_seed()</code> and <code>torch.cuda.manual_seed_all()</code></li> <li>Use <code>torch.Generator</code> for repeatable randomness without global impact</li> <li>Avoid pitfalls in parallel training by seeding workers and managing <code>cudnn</code> flags</li> </ul> <p>Control over randomness is key to reproducibility, debugging, and experimentation.</p>"},{"location":"PartII_overview/#chapter-7-math-operations","title":"Chapter 7: Math Operations","text":"<p>This chapter decodes PyTorch\u2019s math capabilities:</p> <ul> <li>Elementwise math: <code>+</code>, <code>-</code>, <code>*</code>, <code>torch.exp</code>, <code>torch.log</code>, etc.</li> <li>Reduction ops: <code>sum</code>, <code>mean</code>, <code>max</code>, <code>argmax</code>, <code>prod</code></li> <li>Matrix operations: <code>matmul</code>, <code>@</code>, <code>bmm</code>, <code>einsum</code> for advanced contractions</li> <li>Special math: <code>clamp</code>, <code>abs</code>, <code>round</code>, normalization formulas</li> <li>In-place ops and performance tips (with warnings for autograd safety)</li> </ul> <p>Knowing how to express math elegantly and efficiently is a superpower for model builders.</p>"},{"location":"PartII_overview/#chapter-8-broadcasting-and-shape-ops","title":"Chapter 8: Broadcasting and Shape Ops","text":"<p>Shape mismatch errors are a rite of passage. This chapter teaches you how to:</p> <ul> <li>Understand broadcasting rules and how PyTorch aligns mismatched shapes</li> <li>Master reshaping tools: <code>view</code>, <code>reshape</code>, <code>squeeze</code>, <code>unsqueeze</code>, <code>permute</code>, <code>transpose</code></li> <li>Optimize memory with <code>expand()</code> over <code>repeat()</code></li> <li>Apply shape ops to real-world use cases: CNNs, batch processing, and label matching</li> <li>Debug shape bugs with confidence</li> </ul> <p>Broadcasting + shape ops = clean code and fewer runtime nightmares.</p>"},{"location":"PartII_overview/#chapter-9-autograd-and-differentiation","title":"Chapter 9: Autograd and Differentiation","text":"<p>Meet the nervous system of PyTorch:</p> <ul> <li><code>requires_grad=True</code>: mark tensors for gradient tracking</li> <li><code>.backward()</code>: compute gradients through the computation graph</li> <li><code>.grad</code>: access the result of differentiation</li> <li><code>torch.no_grad()</code> and <code>.detach()</code> to freeze parts of your model</li> <li>Build custom gradients with <code>torch.autograd.Function</code></li> <li>Common mistakes (e.g., in-place ops, forgetting <code>zero_()</code>, calling <code>.backward()</code> on non-scalars)</li> </ul> <p>This chapter reveals how PyTorch powers training loops, optimizers, and learning.</p>"},{"location":"PartII_overview/#chapter-10-type-conversions-and-casting","title":"Chapter 10: Type Conversions and Casting","text":"<p>Dtypes and devices must match \u2014 or errors ensue.</p> <ul> <li>Quick casting: <code>.float()</code>, <code>.long()</code>, <code>.bool()</code></li> <li>The mighty <code>.to()</code>: safely cast and move tensors</li> <li><code>.type()</code> vs <code>.to()</code> and why the latter is preferred</li> <li>Safe device transitions and best practices for model portability</li> <li>Common bugs: mismatched dtypes in loss functions, unsafe use of <code>.data</code>, casting silently breaking autograd</li> </ul> <p>Precision matters \u2014 and so does defensive coding with proper casting.</p>"},{"location":"PartII_overview/#summary-of-part-ii","title":"Summary of Part II","text":"Chapter Core Focus Key Skills Gained 4 Tensor Object Creation, reshaping, autograd compatibility 5 Data Types and Devices Precision control, CUDA placement, AMP intro 6 Randomness and Reproducibility Seeding, RNG generators, parallel consistency 7 Math Operations Arithmetic, reductions, matrix math, einsum 8 Broadcasting and Shape Management Shape alignment, expand vs repeat, permute mastery 9 Autograd and Differentiation Gradient tracking, custom gradients, backprop 10 Casting and Type Safety .to(), .float(), device-aware conversion <p>This section gives you full command of torch's deep API layer \u2014 a toolbox you'll use daily for model training, debugging, and performance tuning.</p> <p>\u2192 Coming next: Part III \u2014 Diving into <code>torch.nn</code>, model construction, and building blocks of neural networks.</p>"},{"location":"PartIV_overview/","title":"Part IV: <code>torch</code> in the Real World","text":"<p>You\u2019ve learned tensors, mastered operations, and explored specialized modules. Now it\u2019s time to put it all into practice in real-world workflows.</p> <p>This part is all about performance, deployment readiness, debugging, and integration. Whether you're running models on GPUs, using NumPy, or profiling performance bottlenecks, this section shows you how to keep your models fast, safe, and reliable in production or research settings.</p>"},{"location":"PartIV_overview/#chapter-17-using-torch-with-cuda","title":"Chapter 17: Using <code>torch</code> with CUDA","text":"<p>This chapter unlocks the power of the GPU:</p> <ul> <li>Check if CUDA is available and set the right <code>device</code></li> <li>Move tensors and models with <code>.to(device)</code></li> <li>Use multiple GPUs with <code>DataParallel</code></li> <li>Track memory with <code>memory_allocated()</code> and <code>empty_cache()</code></li> <li>Optimize training with AMP (Automatic Mixed Precision) using <code>autocast()</code> and <code>GradScaler</code></li> <li>Tune performance with <code>torch.backends.cudnn.benchmark</code></li> </ul> <p>\ud83d\udca1 Key Insight: Speed doesn\u2019t just come from <code>.to('cuda')</code> \u2014 managing memory, AMP, and reproducibility settings is just as crucial.</p>"},{"location":"PartIV_overview/#chapter-18-integration-with-numpy","title":"Chapter 18: Integration with NumPy","text":"<p>PyTorch plays beautifully with NumPy:</p> <ul> <li>Convert NumPy \u2192 Tensor: <code>torch.from_numpy(arr)</code></li> <li>Convert Tensor \u2192 NumPy: <code>tensor.numpy()</code> (CPU-only)</li> <li>Shared memory = fast, zero-copy \u2014 but be careful with in-place changes</li> <li>Interfacing with: <code>matplotlib</code>, <code>pandas</code>, OpenCV, SciPy, etc.</li> <li>Safely detach for export: <code>.detach().cpu().numpy()</code></li> </ul> <p>\ud83d\udccc Warning: <code>.numpy()</code> drops autograd history and fails on GPU tensors \u2014 use only in eval, visualization, or exporting.</p>"},{"location":"PartIV_overview/#chapter-19-debugging-profiling-and-best-practices","title":"Chapter 19: Debugging, Profiling, and Best Practices","text":"<p>This is your PyTorch survival guide:</p> <ul> <li>Debug with:</li> <li><code>print(tensor.shape)</code></li> <li><code>torch.isnan()</code>, <code>torch.isinf()</code></li> <li><code>.grad.norm()</code> to catch exploding gradients</li> <li>Catch autograd issues with <code>torch.autograd.set_detect_anomaly(True)</code></li> <li>Profile runtime using <code>torch.profiler.profile()</code> to spot bottlenecks</li> <li>Track GPU memory usage: <code>memory_summary()</code></li> <li>Organize code into <code>model.py</code>, <code>train.py</code>, <code>utils.py</code>, <code>debug.py</code></li> <li>Best practices:</li> <li>Zero gradients every step</li> <li>Avoid <code>.data</code> (use <code>.detach()</code> instead)</li> <li>Use <code>.float()</code> consistently with inputs/targets</li> <li>Assert input/output shapes often</li> </ul> <p>\u2714 Sanity Checklist Included \u2014 the go-to debugging flow for PyTorch practitioners.</p>"},{"location":"PartIV_overview/#summary-of-part-iv","title":"Summary of Part IV","text":"Chapter Focus Area Real-World Benefit 17 CUDA &amp; GPU Acceleration Speed up training, scale to large models 18 NumPy Integration Seamless data exchange with the NumPy ecosystem 19 Debugging &amp; Profiling Build robust, clean, and scalable ML systems <p>Part IV makes you production-ready. You\u2019ll be faster, more efficient, and better equipped to debug and deploy real deep learning applications.</p> <p>\u2192 Next up: Appendices, Cheatsheets, and Cross-Reference Guides for quick access and review.</p>"},{"location":"PartI_overview/","title":"Part I: Getting Started with <code>torch</code>","text":"<p>Welcome to the foundational core of the PyTorch Builder\u2019s Companion Book.</p> <p>This part introduces the torch module\u2014the backbone of the PyTorch ecosystem. Whether you're a beginner eager to understand tensors or an advanced developer optimizing GPU workloads, Part I grounds you in the essential concepts and setup steps required to use PyTorch effectively and confidently.</p>"},{"location":"PartI_overview/#chapter-1-what-is-torch-and-why-does-it-matter","title":"Chapter 1: What is <code>torch</code> and Why Does It Matter?","text":"<p>This chapter lays the philosophical and technical foundation for everything that follows.</p> <ul> <li>torch is not just another module\u2014it's the core engine powering every other component in PyTorch.</li> <li>We explore its place in the PyTorch ecosystem:  </li> <li><code>torch</code> (core tensor ops),  </li> <li><code>torch.nn</code> (neural network abstractions),  </li> <li><code>torch.autograd</code> (differentiation),  </li> <li><code>torchvision/torchaudio/torchtext</code> (domain-specific wrappers).</li> <li>The torch.Tensor is introduced as the primary building block\u2014powerful, GPU-ready, and autograd-compatible.</li> <li>Key takeaway: To truly master PyTorch, you need to understand torch\u2014not just use it, but leverage it for custom layers, debugging, and innovation.</li> </ul>"},{"location":"PartI_overview/#chapter-2-installation-setup","title":"Chapter 2: Installation &amp; Setup","text":"<p>Before building models, you need a proper PyTorch environment.</p> <ul> <li>Walkthrough of the installation process using <code>pip</code> or <code>conda</code> with CPU or CUDA support.</li> <li>Emphasis on using virtual environments to avoid polluting your global Python setup.</li> <li>Quick verification steps:</li> <li>Importing torch</li> <li>Creating random tensors</li> <li>Checking GPU availability</li> <li>Intro to your first mini Tensor Playground\u2014creating, operating, and moving tensors to CUDA if available.</li> <li>Tips on CPU vs GPU behavior, plus troubleshooting advice (e.g., <code>nvidia-smi</code>, Python version compatibility).</li> </ul> <p>By the end of this chapter, you\u2019ll be running tensors on your device and preparing for deeper exploration.</p>"},{"location":"PartI_overview/#chapter-3-tensor-fundamentals","title":"Chapter 3: Tensor Fundamentals","text":"<p>This chapter goes deeper into the anatomy of tensors.</p> <ul> <li>What is a tensor? From scalars (0D) to multi-dimensional batches of data (4D+).</li> <li>Comparison with NumPy:</li> <li>API similarity</li> <li>GPU support</li> <li>Autograd capability</li> <li>Tensor interoperability with NumPy (shared memory warning!).</li> <li>Introduction to device management:</li> <li>Sending tensors to <code>'cpu'</code> or <code>'cuda'</code></li> <li>Choosing a device dynamically</li> <li>Use-case comparisons (CPU vs GPU) and why your tensor placement directly impacts performance.</li> </ul> <p>Mastering tensors means mastering data representation, device flow, and performance\u2014skills critical for both beginners and pros.</p>"},{"location":"PartI_overview/#summary-of-part-i","title":"Summary of Part I","text":"Concept Key Insight <code>torch</code> Module The core engine behind all PyTorch operations <code>torch.Tensor</code> Foundation for all data, models, and gradients in PyTorch Installation &amp; Setup Prepare clean environments with optional CUDA support Device Management Know how to work with CPU vs GPU to optimize performance Tensor Playground Hands-on intro to tensor creation, manipulation, and hardware acceleration <p>Whether you\u2019re debugging deep models or building your first neural net, Part I arms you with essential tools to begin your PyTorch journey on solid ground.</p> <p>\u2192 Next up: A deeper dive into the <code>torch.Tensor</code> API.</p>"},{"location":"Preface/","title":"&nbsp; Preface","text":""},{"location":"Preface/#why-this-book-exists","title":"Why This Book Exists","text":"<p>Most people meet PyTorch through beginner tutorials that work \u2014 until they don\u2019t. You copy a few lines of code, tweak a layer or a tensor, and everything seems fine\u2026 until you hit a cryptic shape mismatch, an exploding loss, or a silent autograd bug that derails your model.</p> <p>This book was created to solve that.</p> <p>The PyTorch Builder\u2019s Companion Book is not just a walkthrough of the <code>torch</code> API \u2014 it\u2019s a builder\u2019s map of PyTorch\u2019s computational engine. I wrote it after months of working directly with <code>torch.Tensor</code>, tracing bugs in gradient flows, and digging into the internals of autograd, CUDA, and advanced training setups.</p> <p>If you\u2019ve ever wondered why your gradients vanish, why <code>.view()</code> fails silently, or how tensors actually move between devices \u2014 this book is for you.</p> <p>This is a guide for those who build things and want PyTorch to feel like a tool, not a black box.</p>"},{"location":"Preface/#who-should-read-this","title":"Who Should Read This","text":"<p>This book is written for:</p> <ul> <li>Engineers and AI builders who want to understand the internals of PyTorch \u2014 not just use <code>nn.Linear</code> and hope it works.</li> <li>TensorFlow or NumPy users transitioning to PyTorch who want deep API fluency.</li> <li>Graduate students or thesis writers looking to customize training loops or create new architectures.</li> <li>Anyone debugging PyTorch code who wants clarity around tensors, devices, gradients, or performance.</li> </ul> <p>You don\u2019t need to be a math wizard. But you do need curiosity \u2014 and a willingness to follow a tensor from shape to shape, from CPU to GPU, and from forward to backward.</p>"},{"location":"Preface/#from-tensors-to-gradients-how-this-book-was-born","title":"From Tensors to Gradients: How This Book Was Born","text":"<p>This book didn\u2019t begin as a textbook. It began as survival notes.</p> <p>Notes on why <code>.detach()</code> works but <code>.data</code> doesn\u2019t. On why <code>view()</code> fails after <code>.permute()</code>. On how to debug silent failures in custom loss functions. On what exactly happens during backpropagation inside PyTorch.</p> <p>Eventually, those notes turned into principles. The principles became diagrams and code. And those became chapters.</p> <p>If you\u2019ve ever tried to freeze part of a pretrained model or troubleshoot why <code>.backward()</code> gives <code>None</code>, you're not alone. This book is a distillation of that confusion \u2014 turned into clarity.</p>"},{"location":"Preface/#what-youll-learn-and-what-you-wont","title":"What You\u2019ll Learn (and What You Won\u2019t)","text":"<p>You will learn:</p> <ul> <li>How <code>torch.Tensor</code> works: creation, reshaping, slicing, and memory layout</li> <li>The difference between <code>float32</code>, <code>float16</code>, <code>bfloat16</code>, and when to use each</li> <li>How autograd builds computation graphs \u2014 and how to debug them</li> <li>How to use <code>torch.nn.functional</code>, <code>torch.fft</code>, <code>torch.linalg</code>, and more</li> <li>Real-world skills: using CUDA, AMP, mixed precision, and multi-GPU training</li> <li>How to write PyTorch code that is readable, robust, and fast</li> </ul> <p>You will not find:</p> <ul> <li>Overly abstract explanations with no runnable code</li> <li>High-level metaphors that gloss over mechanics</li> <li>Tutorials that \"just work\" without explaining why</li> </ul> <p>This is a book about what PyTorch really does \u2014 and how to wield it with precision.</p>"},{"location":"Preface/#how-to-read-this-book-even-if-youre-just-starting-out","title":"How to Read This Book (Even if You\u2019re Just Starting Out)","text":"<p>Each chapter is structured around:</p> <ul> <li>Conceptual Insight \u2013 what this torch API does and why it matters  </li> <li>Code Walkthrough \u2013 annotated, working examples with expected outputs  </li> <li>Debugging Tips \u2013 common edge cases and how to fix them  </li> <li>Practice Prompt \u2013 real tasks to try on your own  </li> <li>Use Case Callout \u2013 where this tool fits into real AI workflows</li> </ul> <p>If you're coming from TensorFlow, focus on the PyTorch equivalents. If you're new to deep learning, lean on the diagrams, visual cues, and hands-on exercises.</p> <p>You don\u2019t need to master everything at once. But by the end, you\u2019ll be able to build, train, debug, and deploy with PyTorch \u2014 not as a user, but as an engineer who understands the framework inside and out.</p> <p>Written and maintained by Clay Mark Sarte</p>"},{"location":"appendix_api_reference/","title":"Appendix C: Full <code>torch</code> API Reference Crosswalk","text":"<p>\u201cAll the power. One list.\u201d</p> Module Link Description <code>torch.Tensor</code> https://pytorch.org/docs/stable/tensors.html Core data structure <code>torch.nn</code> https://pytorch.org/docs/stable/nn.html Layers, loss functions, model building <code>torch.nn.functional</code> https://pytorch.org/docs/stable/nn.functional.html Stateless functional ops <code>torch.autograd</code> https://pytorch.org/docs/stable/autograd.html Gradient tracking, custom ops <code>torch.cuda</code> https://pytorch.org/docs/stable/cuda.html GPU support, memory info <code>torch.utils.data</code> https://pytorch.org/docs/stable/data.html Dataset, DataLoader, Samplers <code>torch.special</code> https://pytorch.org/docs/stable/special.html Advanced math functions (gamma, digamma, etc.) <code>torch.fft</code> https://pytorch.org/docs/stable/fft.html Frequency transforms (fft, rfft, fft2, etc.) <code>torch.linalg</code> https://pytorch.org/docs/stable/linalg.html Modern linear algebra tools <code>torch.profiler</code> https://pytorch.org/docs/stable/profiler.html CPU/GPU performance profiling <code>torch.onnx</code> https://pytorch.org/docs/stable/onnx.html Exporting to ONNX format <code>torch.jit</code> https://pytorch.org/docs/stable/jit.html Scripting &amp; tracing models for speed/export <code>torchvision.transforms</code> https://pytorch.org/vision/stable/transforms.html Image preprocessing <code>torch.utils.tensorboard</code> https://pytorch.org/docs/stable/tensorboard.html Training visualizations"},{"location":"appendix_idioms/","title":"Appendix B: PyTorch Idioms and Gotchas","text":"<p>\u201cRead this before your model does something stupid.\u201d</p>"},{"location":"appendix_idioms/#idioms","title":"Idioms","text":"Goal Idiomatic PyTorch Code Move everything to GPU <code>x = x.to(device); model = model.to(device)</code> Get model predictions <code>with torch.no_grad(): output = model(x)</code> Detach and convert to NumPy <code>x.detach().cpu().numpy()</code> One-hot encode <code>F.one_hot(t.long(), num_classes).float()</code> Check for NaNs <code>torch.isnan(x).any()</code> Log training stats <code>writer.add_scalar('loss', val, step)</code> Save model <code>torch.save(model.state_dict(), 'model.pt')</code> Load model <code>model.load_state_dict(torch.load(...))</code>"},{"location":"appendix_idioms/#gotchas","title":"\u26a0\ufe0f Gotchas","text":"Gotcha What Goes Wrong Fix Using <code>.data</code> for detaching Breaks autograd silently Use <code>.detach()</code> Calling <code>.numpy()</code> on CUDA tensor Immediate crash Move to CPU first In-place ops like <code>+=</code>, <code>add_()</code> May break gradients Use regular ops (<code>x = x + y</code>) Forgetting <code>.train()</code> / <code>.eval()</code> BatchNorm/Dropout misbehave Switch mode explicitly Wrong loss input types Float tensor vs. int labels Match dtypes (<code>.float()</code>) Not zeroing <code>.grad</code> before <code>.backward()</code> Gradients accumulate Call <code>optimizer.zero_grad()</code>"},{"location":"appendix_shapes/","title":"Appendix A: Tensor Shapes Cheat Sheet","text":"<p>\u201cBecause debugging starts with dimensions.\u201d</p> Task / Layer Expected Shape Single image (grayscale) <code>[1, H, W]</code> Single image (RGB) <code>[3, H, W]</code> Batch of grayscale images <code>[B, 1, H, W]</code> Batch of RGB images <code>[B, 3, H, W]</code> Fully connected input <code>[B, features]</code> LSTM input <code>[seq_len, batch, input_size]</code> or <code>[B, T, F]</code> (<code>batch_first=True</code>) Transformer input <code>[B, seq_len]</code> or <code>[B, seq_len, emb_dim]</code> <code>nn.Embedding</code> input <code>[B, T]</code> (int64 tokens) \u2192 Output: <code>[B, T, emb_dim]</code> CNN output to FC layer <code>[B, C, H, W]</code> \u2192 <code>x.view(B, -1)</code> Classification output <code>[B, num_classes]</code> Regression output <code>[B, 1]</code> or <code>[B]</code> <p>Always <code>print(.shape)</code> at each step of your model to avoid dimensional disasters.</p>"},{"location":"chapter10_type_casting/","title":"Chapter 10: Type Conversions and Casting","text":"<p>\u201cWrong dtype, wrong device \u2014 game over.\u201d</p>"},{"location":"chapter10_type_casting/#101-why-casting-matters","title":"\ud83d\udd04 10.1 Why Casting Matters","text":"<p>Tensor operations require matching types and devices. If not, you\u2019ll face runtime errors like: <pre><code>RuntimeError: expected scalar type Float but found Long\n</code></pre></p> <p>Or worse \u2014 silently incorrect results.</p> <p>So casting properly is not just about avoiding bugs \u2014 it\u2019s about making your models work as intended.</p>"},{"location":"chapter10_type_casting/#102-basic-casting-methods","title":"10.2 Basic Casting Methods","text":""},{"location":"chapter10_type_casting/#convert-to-float-long-int-bool","title":"\u27a4 Convert to Float, Long, Int, Bool","text":"<p><pre><code>x = torch.tensor([1, 0, 1])\nx.float()      # torch.float32\nx.long()       # torch.int64\nx.int()        # torch.int32\nx.bool()       # torch.bool\n</code></pre> These are shorthand wrappers around .to(dtype=...).</p>"},{"location":"chapter10_type_casting/#103-to-the-multipurpose-transformer","title":"10.3 .to() \u2014 The Multipurpose Transformer","text":"<p><code>.to()</code> can change: - The dtype - The device - Or both at once</p> <pre><code>x = x.to(torch.float32)\nx = x.to('cuda')\nx = x.to(torch.float16, device='cuda')\n</code></pre> <p>\u2705 Best practice: use .to() for cross-device + dtype-safe conversions in pipelines.</p>"},{"location":"chapter10_type_casting/#104-type-less-flexible-still-useful","title":"10.4 .type() \u2014 Less Flexible, Still Useful","text":"<pre><code>x = x.type(torch.FloatTensor)\n</code></pre> <p>But it doesn\u2019t support device changes, so <code>.to()</code> is preferred in most modern PyTorch code.</p>"},{"location":"chapter10_type_casting/#105-matching-types-in-ops","title":"10.5 Matching Types in Ops","text":"<p>This error is extremely common when dealing with losses or metrics: <pre><code>preds = torch.tensor([0.6, 0.2, 0.8])         # float32\nlabels = torch.tensor([1, 0, 1])              # int64\nloss = torch.nn.BCELoss()\nloss(preds, labels)   # \u274c Throws error\n</code></pre></p>"},{"location":"chapter10_type_casting/#fix","title":"\u2705 Fix:","text":"<pre><code>labels = labels.float()\n</code></pre> <p>Most loss functions expect both inputs to be <code>float32</code>, not integers.</p>"},{"location":"chapter10_type_casting/#106-one-hot-encoding-pitfall","title":"10.6 One-Hot Encoding Pitfall","text":"<p>When using one-hot encodings, make sure your tensors are in <code>float</code> (or <code>bool</code> in some masking scenarios): <pre><code>y = torch.tensor([0, 2])\none_hot = torch.nn.functional.one_hot(y, num_classes=3).float()\n</code></pre></p> <p>If you pass long-typed one-hot vectors into models, expect weird gradients or silent failures.</p>"},{"location":"chapter10_type_casting/#107-converting-between-devices-safely","title":"10.7 Converting Between Devices Safely","text":"<p>Avoid this: <pre><code>x.cuda()\n</code></pre> Use this: <pre><code>x.to('cuda')  # Safer, cleaner, can be combined with dtype\n</code></pre> To stay cross-platform, define your device once: <pre><code>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nx = x.to(device)\n</code></pre></p>"},{"location":"chapter10_type_casting/#108-dangerous-casting-practices","title":"\ud83d\udeab 10.8 Dangerous Casting Practices","text":"Practice Why It's Bad Fix Using <code>.data</code> Bypasses autograd Use <code>.detach()</code> Forgetting <code>.float()</code> Breaks loss functions Always match model dtype Manual <code>.cuda()</code> calls Breaks portability Use <code>.to(device)</code> Mixing float64 + float32 Silently slows ops Use consistent dtype"},{"location":"chapter10_type_casting/#109-summary","title":"10.9 Summary","text":"<ul> <li> <p>Use <code>.float()</code>, <code>.long()</code>, <code>.bool()</code> for quick casting.</p> </li> <li> <p>Use .<code>to(dtype, device)</code> to control both type and placement.</p> </li> <li> <p>Prefer <code>.to()</code> over <code>.type()</code> and never use <code>.data</code>.</p> </li> <li> <p>Always check input/output types before applying loss or activation functions.</p> </li> <li> <p>Define your device once and pass it around to keep your code portable.</p> </li> </ul>"},{"location":"chapter11_linalg/","title":"Chapter 11: <code>torch.linalg</code>","text":"<p>\u201cWith great matrices comes great responsibility.\u201d</p>"},{"location":"chapter11_linalg/#111-what-is-torchlinalg","title":"11.1 What is <code>torch.linalg</code>?","text":"<p><code>torch.linalg</code> is PyTorch\u2019s modern linear algebra module, introduced to offer more stable, consistent, and NumPy-compatible operations compared to older functions like <code>torch.svd()</code> or <code>torch.eig()</code>.</p> <p>It handles:</p> <ul> <li>Matrix decompositions  </li> <li>Solvers  </li> <li>Norms  </li> <li>Eigenvalues  </li> <li>Inverses  </li> <li>Determinants  </li> </ul> <p>\ud83d\udca1 Bonus: most functions support batched operations, autograd, and GPU acceleration.</p>"},{"location":"chapter11_linalg/#112-matrix-inversion-and-solving-linear-systems","title":"11.2 Matrix Inversion and Solving Linear Systems","text":""},{"location":"chapter11_linalg/#invert-a-matrix","title":"\u27a4 Invert a matrix","text":"<pre><code>A = torch.randn(3, 3)\nA_inv = torch.linalg.inv(A)\n</code></pre> <p>\u26a0\ufe0f Inverting is expensive. If you're solving <code>Ax = b</code>, use a solver instead.</p>"},{"location":"chapter11_linalg/#solve-linear-system-ax-b","title":"\u27a4 Solve linear system <code>Ax = b</code>","text":"<pre><code>A = torch.tensor([[3.0, 1.0], [1.0, 2.0]])\nb = torch.tensor([9.0, 8.0])\nx = torch.linalg.solve(A, b)\n</code></pre> <p>\u2705 Preferred over computing inv(A) @ b \u2014 more numerically stable.</p>"},{"location":"chapter11_linalg/#113-matrix-determinant-and-rank","title":"11.3 Matrix Determinant and Rank","text":""},{"location":"chapter11_linalg/#determinant","title":"\u27a4 Determinant","text":"<pre><code>A = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\ntorch.linalg.det(A)\n</code></pre>"},{"location":"chapter11_linalg/#matrix-rank","title":"\u27a4 Matrix Rank","text":"<pre><code>torch.linalg.matrix_rank(A)\n</code></pre> <p>Useful for checking if a matrix is invertible or has full column rank.</p>"},{"location":"chapter11_linalg/#114-norms-and-condition-numbers","title":"11.4 Norms and Condition Numbers","text":""},{"location":"chapter11_linalg/#norms","title":"\u27a4 Norms","text":"<pre><code>x = torch.tensor([3.0, 4.0])\ntorch.linalg.norm(x)             # L2 norm\n\nA = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\ntorch.linalg.norm(A, ord='fro')  # Frobenius norm\n</code></pre> <p>Other <code>ord</code> options: <code>'nuc'</code> (nuclear), <code>1</code>, <code>2</code>, <code>inf</code>, <code>-inf</code></p>"},{"location":"chapter11_linalg/#condition-number","title":"\u27a4 Condition Number","text":"<pre><code>torch.linalg.cond(A)\n</code></pre> <p>High condition numbers indicate numerical instability \u2014 useful for diagnostics.</p>"},{"location":"chapter11_linalg/#115-eigenvalues-and-eigenvectors","title":"11.5 Eigenvalues and Eigenvectors","text":""},{"location":"chapter11_linalg/#eigendecomposition","title":"\u27a4 Eigendecomposition","text":"<pre><code>A = torch.tensor([[5.0, 4.0], [1.0, 2.0]])\neigenvalues, eigenvectors = torch.linalg.eig(A)\n</code></pre> <p>\u26a0\ufe0f eigenvalues may be complex64 or complex128.</p>"},{"location":"chapter11_linalg/#for-symmetrichermitian-matrices","title":"\u27a4 For symmetric/Hermitian matrices:","text":"<pre><code>eigenvalues = torch.linalg.eigvalsh(A)  # Faster and more stable\n</code></pre>"},{"location":"chapter11_linalg/#116-svd-singular-value-decomposition","title":"11.6 SVD (Singular Value Decomposition)","text":"<p>Useful in: - PCA - Low-rank approximation - Image compression <pre><code>U, S, Vh = torch.linalg.svd(A, full_matrices=False)\nA_reconstructed = U @ torch.diag(S) @ Vh\n</code></pre></p>"},{"location":"chapter11_linalg/#117-qr-and-lu-decomposition","title":"11.7 QR and LU Decomposition","text":""},{"location":"chapter11_linalg/#qr-decomposition","title":"\u27a4 QR decomposition","text":"<pre><code>Q, R = torch.linalg.qr(A)\n</code></pre>"},{"location":"chapter11_linalg/#lu-decomposition","title":"\u27a4 LU decomposition","text":"<pre><code>LU, pivots = torch.linalg.lu_factor(A)\n</code></pre>"},{"location":"chapter11_linalg/#118-relationship-with-numpy","title":"11.8 Relationship with NumPy","text":"<p>PyTorch\u2019s <code>torch.linalg</code> mirrors <code>numpy.linalg</code> in: - Function names - Shape conventions - Numerical semantics</p> <p>But PyTorch: - \u2705 Supports autograd - \u2705 Runs on GPU - \u2705 Handles batch operations  </p> <p>You can almost always port NumPy linear algebra code directly to PyTorch with minimal edits.</p>"},{"location":"chapter11_linalg/#119-summary","title":"\u2705 11.9 Summary","text":"Operation Function Matrix inverse torch.linalg.inv() Solve Ax = b torch.linalg.solve() Determinant torch.linalg.det() Eigenvalues/vectors torch.linalg.eig() SVD torch.linalg.svd() Norm torch.linalg.norm() R / LU torch.linalg.qr(), lu() <ul> <li> <p>Use torch.linalg for numerically stable, batched, and autograd-compatible operations.</p> </li> <li> <p>Avoid inv() when you can use solve().</p> </li> <li> <p>Most functions support GPU \u2014 just move your tensors to cuda.</p> </li> </ul>"},{"location":"chapter12_nn_functional/","title":"Chapter 12: <code>torch.nn.functional</code> and Activation Math","text":"<p>\u201cWhen you don\u2019t need a layer, you just need a function.\u201d</p>"},{"location":"chapter12_nn_functional/#121-what-is-torchnnfunctional","title":"12.1 What is <code>torch.nn.functional</code>?","text":"<p>While <code>torch.nn</code> contains layer classes like <code>nn.ReLU</code>, <code>nn.Linear</code>, etc., the <code>torch.nn.functional</code> module gives you stateless functional versions.</p> Type Example from <code>torch.nn.functional</code> Activation functions <code>F.relu</code>, <code>F.sigmoid</code>, <code>F.softmax</code> Loss functions <code>F.cross_entropy</code>, <code>F.mse_loss</code> Convolutional ops <code>F.conv2d</code>, <code>F.max_pool2d</code> Normalization <code>F.batch_norm</code>, <code>F.layer_norm</code> Utility transforms <code>F.pad</code>, <code>F.interpolate</code>, <code>F.one_hot</code> <p>Stateless means you must pass all arguments explicitly \u2014 no hidden weights or buffers.</p>"},{"location":"chapter12_nn_functional/#122-common-activation-functions","title":"12.2 Common Activation Functions","text":"<pre><code>import torch.nn.functional as F\nx = torch.tensor([-1.0, 0.0, 1.0])\n</code></pre>"},{"location":"chapter12_nn_functional/#relu","title":"\u27a4 ReLU","text":"<pre><code>F.relu(x)  # tensor([0., 0., 1.])\n</code></pre>"},{"location":"chapter12_nn_functional/#sigmoid","title":"\u27a4 Sigmoid","text":"<pre><code>F.sigmoid(x)  # tensor([0.2689, 0.5000, 0.7311])\n</code></pre>"},{"location":"chapter12_nn_functional/#tanh","title":"\u27a4 Tanh","text":"<pre><code>F.tanh(x)  # tensor([-0.7616,  0.0000,  0.7616])\n</code></pre>"},{"location":"chapter12_nn_functional/#softmax","title":"\u27a4 Softmax","text":"<pre><code>logits = torch.tensor([2.0, 1.0, 0.1])\nF.softmax(logits, dim=0)  # Probabilities that sum to 1\n</code></pre> <p>\u2705 Always specify <code>dim</code> with <code>softmax</code>.</p>"},{"location":"chapter12_nn_functional/#123-loss-functions-in-functional","title":"12.3 Loss Functions in Functional","text":"<p>Loss functions in F behave like their nn counterparts \u2014 but are stateless.</p>"},{"location":"chapter12_nn_functional/#cross-entropy-loss","title":"\u27a4 Cross-Entropy Loss","text":"<pre><code>logits = torch.tensor([[2.0, 1.0, 0.1]])\ntargets = torch.tensor([0])\nloss = F.cross_entropy(logits, targets)\n</code></pre> <p>F.cross_entropy() = log_softmax + nll_loss internally. So pass raw logits, not softmaxed outputs.</p>"},{"location":"chapter12_nn_functional/#mse-loss","title":"\u27a4 MSE Loss","text":"<pre><code>pred = torch.tensor([0.5, 0.7])\ntarget = torch.tensor([1.0, 0.0])\nF.mse_loss(pred, target)\n</code></pre>"},{"location":"chapter12_nn_functional/#binary-cross-entropy","title":"\u27a4 Binary Cross-Entropy","text":"<pre><code>F.binary_cross_entropy(torch.sigmoid(pred), target)\n</code></pre>"},{"location":"chapter12_nn_functional/#124-functional-vs-nnmodule","title":"12.4 <code>functional</code> vs <code>nn.Module</code>","text":"Situation Use You need modularity <code>nn.ReLU()</code>, <code>nn.Linear()</code> You want fine-grained control <code>F.relu()</code>, <code>F.linear()</code> Inside <code>forward()</code> method Prefer<code>F.*</code> for functions Initializing outside model Use <code>nn.Module</code> versions"},{"location":"chapter12_nn_functional/#example","title":"Example:","text":"<pre><code># With Module\nself.relu = nn.ReLU()\nx = self.relu(x)\n\n# With Functional\nimport torch.nn.functional as F\nx = F.relu(x)\n</code></pre> <p>Inside <code>forward()</code>, many developers prefer <code>F.*</code> to keep the model class minimal and explicit.</p>"},{"location":"chapter12_nn_functional/#125-functional-layers-linear-conv-etc","title":"12.5 Functional Layers (Linear, Conv, etc.)","text":""},{"location":"chapter12_nn_functional/#functional-linear-layer","title":"Functional Linear Layer:","text":"<pre><code>weight = torch.randn(10, 5)\nbias = torch.randn(10)\nx = torch.randn(1, 5)\nF.linear(x, weight, bias)\n</code></pre> <p>You're responsible for managing parameters manually. Useful for: - Writing custom layers - Doing meta-learning - Implementing custom architectures</p>"},{"location":"chapter12_nn_functional/#126-one-hot-encoding","title":"12.6 One-Hot Encoding","text":"<pre><code>labels = torch.tensor([0, 2])\nF.one_hot(labels, num_classes=3).float()\n</code></pre> <p>Perfect for manual cross-entropy implementations or label smoothing.</p>"},{"location":"chapter12_nn_functional/#127-other-useful-functions","title":"12.7 Other Useful Functions","text":""},{"location":"chapter12_nn_functional/#padding","title":"\u27a4 Padding:","text":"<pre><code>F.pad(x, pad=(1, 1), mode='constant', value=0)\n</code></pre>"},{"location":"chapter12_nn_functional/#upsampling-interpolation","title":"\u27a4 Upsampling / Interpolation:","text":"<pre><code>F.interpolate(image, scale_factor=2, mode='bilinear')\n</code></pre>"},{"location":"chapter12_nn_functional/#128-caution-dont-mix-modules-with-functionals-blindly","title":"12.8 Caution: Don\u2019t Mix Modules with Functionals Blindly","text":"<p>Mixing <code>nn.CrossEntropyLoss()</code> with <code>F.softmax()</code>? \u274c Bad idea.</p> <p><code>nn.CrossEntropyLoss()</code> expects raw logits. Passing softmaxed values will double-softmax your output and lead to training instability.</p>"},{"location":"chapter12_nn_functional/#129-summary","title":"\u2705 12.9 Summary","text":"Category Functional API Examples Activations F.relu, F.softmax, F.tanh Loss Functions F.cross_entropy, F.mse_loss Layer Ops F.linear, F.conv2d, F.pad <ul> <li>torch.nn.functional is stateless and explicit.</li> <li>Great for flexibility, custom layers, or experimental architectures.</li> <li>Use with care \u2014 you must manage shapes, devices, and parameters manually.</li> </ul>"},{"location":"chapter13_special/","title":"Chapter 13: <code>torch.special</code>","text":"<p>\u201cWhen you need more than just relu... enter special ops.\u201d</p>"},{"location":"chapter13_special/#131-what-is-torchspecial","title":"13.1 What is <code>torch.special</code>?","text":"<p><code>torch.special</code> contains advanced mathematical functions not included in the standard tensor API. These are often used in:</p> <ul> <li>Statistical distributions (e.g., gamma, beta)  </li> <li>Numerical analysis  </li> <li>Scientific modeling  </li> <li>Custom loss/activation layers  </li> <li>Deep probabilistic models (e.g., variational inference)</li> </ul> <p>This module aligns closely with SciPy\u2019s <code>scipy.special</code>.</p>"},{"location":"chapter13_special/#132-common-functions-in-torchspecial","title":"13.2 Common Functions in <code>torch.special</code>","text":"<p>Let\u2019s walk through the major ones \u2014 with context on when you might actually need them.</p>"},{"location":"chapter13_special/#torchspecialexpit-sigmoid","title":"\ud83d\udd39 <code>torch.special.expit()</code> \u2014 Sigmoid","text":"<pre><code>x = torch.tensor([-2.0, 0.0, 2.0])\ntorch.special.expit(x)\n# tensor([0.1192, 0.5000, 0.8808])\n</code></pre> <p>This is numerically stable and equivalent to: <code>1 / (1 + torch.exp(-x))</code>  \u2705 Use this when implementing binary logistic regression manually.</p>"},{"location":"chapter13_special/#torchspecialerf-and-erfc-error-functions","title":"\ud83d\udd39 <code>torch.special.erf()</code> and <code>erfc()</code> \u2014 Error Functions","text":"<p><pre><code>torch.special.erf(torch.tensor([0.0, 1.0, 2.0]))\n# tensor([0.0000, 0.8427, 0.9953])\n</code></pre> Used in: - Gaussian distributions - Signal processing - Probabilistic functions in physics/finance</p> <p><code>erfc(x)</code> = <code>1 - erf(x)</code></p>"},{"location":"chapter13_special/#torchspecialgamma-and-lgamma","title":"\ud83d\udd39 <code>torch.special.gamma()</code> and <code>lgamma()</code>","text":"<p><pre><code>torch.special.gamma(torch.tensor([1.0, 2.0, 3.0]))   # \u2192 [1, 1, 2]\ntorch.special.lgamma(torch.tensor([1.0, 2.0, 3.0]))  # log(gamma(x))\n</code></pre> Used in: - Generalized distributions - Bayesian models - Reinforcement learning algorithms</p> <p>lgamma is useful to avoid underflow/overflow when multiplying large factorial terms.</p>"},{"location":"chapter13_special/#torchspecialdigamma-and-polygamma","title":"\ud83d\udd39 <code>torch.special.digamma()</code> and <code>polygamma()</code>","text":"<p><pre><code>torch.special.digamma(torch.tensor([1.0, 2.0, 3.0]))\n</code></pre> - <code>digamma(x)</code> is the derivative of <code>log(gamma(x))</code></p> <ul> <li>p<code>olygamma(n, x)</code> gives the n-th derivative</li> </ul> <p>Useful in variational inference, Dirichlet models, and Bayesian updates.</p>"},{"location":"chapter13_special/#torchspeciali0-modified-bessel-function-1st-kind","title":"\ud83d\udd39 <code>torch.special.i0()</code> \u2014 Modified Bessel Function (1st Kind)","text":"<p><pre><code>torch.special.i0(torch.tensor([0.0, 1.0, 2.0]))\n</code></pre> Used in: - Waveform analysis - Physics simulations - Signal modeling</p>"},{"location":"chapter13_special/#torchspecialxlogyx-y-stable-x-logy","title":"\ud83d\udd39 <code>torch.special.xlogy(x, y)</code> \u2014 Stable <code>x * log(y)</code>","text":"<pre><code>x = torch.tensor([0.0, 1.0])\ny = torch.tensor([0.5, 0.5])\ntorch.special.xlogy(x, y)\n</code></pre> <p>Handles 0 * log(0) safely  Used in KL divergence and entropy calculations \u2014 avoids NaNs.</p>"},{"location":"chapter13_special/#133-why-these-matter-for-deep-learning","title":"13.3 Why These Matter for Deep Learning","text":"Use Case Function(s) Implementing custom loss <code>xlogy</code>, <code>lgamma</code>, <code>digamma</code> Variational Inference <code>digamma</code>, <code>polygamma</code>, <code>gamma</code> Sampling from complex distributions <code>gamma</code>, <code>erf</code>, <code>i0</code> Numerical stability <code>lgamma</code>, <code>xlogy</code> <p>If you're working beyond basic supervised learning \u2014 into generative models, Bayesian networks, or scientific ML \u2014 these are vital.</p>"},{"location":"chapter13_special/#134-caution-stability-and-edge-cases","title":"\u26a0\ufe0f 13.4 Caution: Stability and Edge Cases","text":"<ul> <li>Many of these functions have singularities (e.g., <code>digamma(0) = -inf</code>)</li> <li>Use <code>.float()</code> or <code>.double()</code> \u2014 some special ops may not support <code>half()</code> or <code>bfloat16</code></li> <li>Combine with <code>torch.clamp()</code> to avoid domain errors</li> </ul>"},{"location":"chapter13_special/#135-summary","title":"\u2705 13.5 Summary","text":"Function Description expit Sigmoid (numerically stable) erf, erfc Gaussian integrals gamma, lgamma Generalized factorials, log-safe digamma, poly* Derivatives of gamma/log-gamma i0 Bessel function (signal theory) xlogy Safe x * log(y) computation <ul> <li> <p>torch.special is a power toolkit for building mathematically correct models</p> </li> <li> <p>Used in advanced, probabilistic, or physics-based modeling</p> </li> <li> <p>If you're using KL divergence, entropy, or variational methods \u2014 this chapter is essential</p> </li> </ul>"},{"location":"chapter14_fft/","title":"Chapter 14: <code>torch.fft</code>","text":"<p>\u201cWhen you leave time behind and think in frequencies.\u201d</p>"},{"location":"chapter14_fft/#141-what-is-torchfft","title":"14.1 What is <code>torch.fft</code>?","text":"<p>The <code>torch.fft</code> module brings PyTorch into the frequency domain, letting you:</p> <ul> <li>Decompose signals (like audio, images) into sine/cosine waves  </li> <li>Denoise data  </li> <li>Detect periodic patterns  </li> <li>Power audio processing, computer vision, and even quantum simulations  </li> </ul> <p>It\u2019s the PyTorch equivalent of NumPy\u2019s <code>np.fft</code> and is built on highly optimized backend code (MKL, CUFFT).</p>"},{"location":"chapter14_fft/#142-forward-and-inverse-fft","title":"14.2 Forward and Inverse FFT","text":"<p>The most basic use case: go to frequency space, and come back.</p>"},{"location":"chapter14_fft/#1d-fft-and-ifft","title":"\u27a4 1D FFT and IFFT","text":"<p><pre><code>import torch.fft\nx = torch.randn(8)\nX = torch.fft.fft(x)                 # Frequency domain (complex numbers)\nx_reconstructed = torch.fft.ifft(X)  # Back to time domain\n</code></pre> Result is a complex tensor: real + imaginary parts</p> <p><code>X.real</code>, <code>X.imag</code></p>"},{"location":"chapter14_fft/#143-real-ffts-rfft-and-irfft","title":"14.3 Real FFTs: <code>rfft()</code> and <code>irfft()</code>","text":"<p>If your input is real-valued (like audio), use real FFTs for speed: <pre><code>x = torch.randn(8)\nX = torch.fft.rfft(x)           # Faster, optimized for real input\nx_rec = torch.fft.irfft(X, n=8) # Reconstruct original signal\n</code></pre> - <code>rfft()</code> reduces output size by ~50% - <code>irfft()</code> needs n (original signal length)</p>"},{"location":"chapter14_fft/#144-2d-ffts-hello-images","title":"14.4 2D FFTs \u2014 Hello, Images","text":"<p>Use <code>fft2()</code> and <code>ifft2()</code> to process 2D signals (images, heatmaps, etc.) <pre><code>img = torch.randn(128, 128)\nF_img = torch.fft.fft2(img)\nimg_back = torch.fft.ifft2(F_img).real  # Often drop imaginary part\n</code></pre></p> <p>You can even apply frequency masks (e.g., blur, sharpen, edge-detect) directly in frequency space.</p>"},{"location":"chapter14_fft/#145-common-functions-in-torchfft","title":"14.5 Common Functions in torch.fft","text":"Function Description fft() N-point FFT ifft() Inverse FFT rfft() Real-input FFT irfft() Inverse of real FFT fft2(), ifft2() 2D FFT and inverse fftn() N-dimensional FFT"},{"location":"chapter14_fft/#146-use-cases-of-fft-in-deep-learning","title":"14.6 Use Cases of FFT in Deep Learning","text":"Application Why FFT? Audio analysis Detect pitch, noise, rhythm Image filtering Frequency-based blurs or edges Signal denoising Filter out high-frequency noise Physics/finance models     T time-to-frequency domain switching Neural net acceleration Multiply in freq space (FFT Conv) <p>Spectral ConvNets? Yep \u2014 they multiply weights in the frequency domain.</p>"},{"location":"chapter14_fft/#147-caveats-and-complex-tensor-handling","title":"\u26a0\ufe0f 14.7 Caveats and Complex Tensor Handling","text":"<ul> <li>Most FFT results are complex tensors <pre><code>x = torch.fft.fft(signal)\nmagnitude = x.abs()\nphase = x.angle()\n</code></pre></li> <li>ifft() should return back to your original domain \u2014 but may differ slightly due to floating-point precision</li> <li>rfft() and irfft() require careful dimension tracking</li> </ul>"},{"location":"chapter14_fft/#148-example-denoising-a-signal-with-fft","title":"14.8 Example: Denoising a Signal with FFT","text":"<pre><code>import torch\n# Create noisy sine wave\nt = torch.linspace(0, 1, 500)\nsignal = torch.sin(2 * torch.pi * 5 * t) + 0.5 * torch.randn_like(t)\n\n# FFT\nF_signal = torch.fft.fft(signal)\n\n# Zero out high frequencies\nF_filtered = F_signal.clone()\nF_filtered[50:-50] = 0  # Keep low-frequency content\n\n# IFFT to recover signal\ndenoised = torch.fft.ifft(F_filtered).real\n</code></pre> <p>You just built a basic low-pass filter using PyTorch. \ud83d\ude0e</p>"},{"location":"chapter14_fft/#149-summary","title":"\u2705 14.9 Summary","text":"Task Use This 1D signal analysis fft, rfft, ifft Image processing fft2, ifft2 Speed + real input rfft, irfft Custom filters Modify FFT result, then ifft Neural speedups Spectral convolutions <ul> <li> <p>torch.fft brings NumPy-level spectral power into the PyTorch ecosystem</p> </li> <li> <p>All FFT outputs are complex tensors \u2014 handle real/imag wisely</p> </li> <li> <p>Use this for audio, images, denoising, and modeling periodic signals</p> </li> </ul>"},{"location":"chapter15_utils/","title":"Chapter 15: <code>torch.utils</code>","text":"<p>\u201cWithout utils, it\u2019s just you and a for-loop in the wilderness.\u201d</p>"},{"location":"chapter15_utils/#151-what-is-torchutils","title":"15.1 What is <code>torch.utils</code>?","text":"<p><code>torch.utils</code> is a collection of essential utilities that make PyTorch practical for real-world training:</p> <ul> <li><code>data</code>: Dataset &amp; DataLoader interface  </li> <li><code>tensorboard</code>: Visualize training progress  </li> <li><code>checkpoint</code>: Save &amp; resume models / reduce memory  </li> <li><code>cpp_extension</code>, <code>throughput_benchmark</code>, etc.</li> </ul> <p>We\u2019ll focus on the most important parts you\u2019ll use almost daily.</p>"},{"location":"chapter15_utils/#152-torchutilsdata-custom-datasets-and-dataloaders","title":"15.2 <code>torch.utils.data</code> \u2014 Custom Datasets and DataLoaders","text":"<p>This submodule is the backbone of PyTorch\u2019s training loop.</p>"},{"location":"chapter15_utils/#create-a-custom-dataset","title":"\u27a4 Create a Custom Dataset","text":"<pre><code>from torch.utils.data import Dataset\n\nclass MyDataset(Dataset):\n    def __init__(self, data, labels):\n        self.data = data\n        self.labels = labels\n\n    def __getitem__(self, index):\n        return self.data[index], self.labels[index]\n\n    def __len__(self):\n        return len(self.data)\n</code></pre>"},{"location":"chapter15_utils/#use-with-dataloader","title":"\u27a4 Use with DataLoader","text":"<pre><code>from torch.utils.data import DataLoader\n\ndataset = MyDataset(torch.randn(100, 3), torch.randint(0, 2, (100,)))\nloader = DataLoader(dataset, batch_size=16, shuffle=True)\n\nfor batch_data, batch_labels in loader:\n    pass  # training loop here\n</code></pre> <p>\u2705 DataLoader handles batching, shuffling, multiprocessing (num_workers), and pinning memory to improve performance.</p>"},{"location":"chapter15_utils/#153-built-in-helpers","title":"15.3 Built-in Helpers","text":""},{"location":"chapter15_utils/#tensordataset-wrap-tensors-directly","title":"\u27a4 TensorDataset \u2014 Wrap tensors directly","text":"<pre><code>from torch.utils.data import TensorDataset\n\nds = TensorDataset(torch.randn(100, 3), torch.randint(0, 2, (100,)))\n</code></pre>"},{"location":"chapter15_utils/#concatdataset-subset-randomsampler","title":"\u27a4 <code>ConcatDataset</code>, <code>Subset</code>, <code>RandomSampler</code>","text":"<p>These let you:</p> <ul> <li>Combine datasets (<code>ConcatDataset</code>)</li> <li>Take slices (<code>Subset</code>)</li> <li>Customize sample orders (<code>Sampler</code>, <code>WeightedRandomSampler</code>)</li> </ul>"},{"location":"chapter15_utils/#154-torchutilstensorboard-visualize-your-training","title":"15.4 <code>torch.utils.tensorboard</code> \u2014 Visualize Your Training","text":""},{"location":"chapter15_utils/#track-loss-accuracy-gradients","title":"\u27a4 Track loss, accuracy, gradients:","text":"<p><pre><code>from torch.utils.tensorboard import SummaryWriter\n\nwriter = SummaryWriter()\n\nfor epoch in range(10):\n    train_loss = 0.5 * (10 - epoch)\n    writer.add_scalar('Loss/train', train_loss, epoch)\n    writer.add_histogram('Weights/layer1', model.layer1.weight, epoch)\n\nwriter.flush()\n</code></pre> Then launch with: <pre><code>tensorboard --logdir=runs/\n</code></pre></p> <p>Great for comparing experiments, visualizing embeddings, and debugging weird training plateaus.</p>"},{"location":"chapter15_utils/#55-torchutilscheckpoint-save-memory-with-recompute","title":"5.5 <code>torch.utils.checkpoint</code> \u2014 Save Memory with Recompute","text":"<p>Use this to trade compute for memory in huge models like Transformers or ResNets.</p>"},{"location":"chapter15_utils/#wrap-part-of-the-forward-pass","title":"\u27a4 Wrap part of the forward pass:","text":"<p><pre><code>from torch.utils.checkpoint import checkpoint\n\ndef custom_forward(*inputs):\n    return model(*inputs)\n\noutput = checkpoint(custom_forward, x)\n</code></pre> - PyTorch will discard intermediate tensors during forward()</p> <ul> <li>Recompute them during .backward() to save GPU memory</li> </ul>"},{"location":"chapter15_utils/#156-other-utilities","title":"15.6 Other Utilities","text":""},{"location":"chapter15_utils/#throughput_benchmark","title":"\u27a4 <code>throughput_benchmark</code>","text":"<p>Used to test model speed under realistic loads.</p>"},{"location":"chapter15_utils/#cpp_extension","title":"\u27a4 <code>cpp_extension</code>","text":"<p>Compile and call custom CUDA/C++ kernels from Python \u2014 used in: - Detectron2 - Hugging Face Transformers - Other low-level optimization libraries</p> <p>Very advanced \u2014 only needed if you\u2019re building custom operators or native extensions.</p>"},{"location":"chapter15_utils/#157-real-world-workflow-using-torchutils","title":"15.7 Real-World Workflow Using <code>torch.utils</code>","text":"Task Tool Used Batch loading <code>DataLoader</code> Custom data pipeline <code>Dataset</code> subclass Visual logging <code>tensorboard.SummaryWriter</code> Save VRAM in deep nets <code>checkpoint()</code> Combine multiple datasets <code>ConcatDataset</code>, <code>Subset</code> <p>\u2705 These are the unsung heroes of PyTorch \u2014 not mathematical, but without them, every model would be a pain to train, debug, or scale.</p>"},{"location":"chapter15_utils/#158-summary","title":"\u2705 15.8 Summary","text":"Submodule Use Case <code>data</code> Datasets, DataLoaders, batching <code>tensorboard</code> Visualize metrics, histograms, images <code>checkpoint</code> Memory-efficient training <code>cpp_extension</code> Write custom kernels (advanced) <ul> <li> <p><code>torch.utils</code> turns raw PyTorch into a full-blown ML framework</p> </li> <li> <p>You\u2019ll use <code>data</code> and <code>tensorboard</code> in every project</p> </li> <li> <p>Use <code>checkpoint</code> when running out of VRAM or fitting giant nets</p> </li> </ul>"},{"location":"chapter16_storage_memory/","title":"Chapter 16: <code>torch.storage</code>, <code>torch.memory_format</code>, and Low-Level Tensor Memory","text":"<p>\u201cBecause knowing how your tensors are stored can save you time, memory, and sanity.\u201d</p>"},{"location":"chapter16_storage_memory/#161-what-is-torchstorage","title":"16.1 What is <code>torch.storage</code>?","text":"<p>At its core, every PyTorch Tensor is a view into a Storage object \u2014 a 1D, contiguous block of memory that holds all the actual data.</p>"},{"location":"chapter16_storage_memory/#access-the-underlying-storage","title":"\u27a4 Access the underlying storage:","text":"<p><pre><code>x = torch.tensor([[1, 2], [3, 4]])\nx_storage = x.storage()\nprint(list(x_storage))  # Output: [1, 2, 3, 4]\n</code></pre> x[0][1] accesses the second element, but it\u2019s ultimately referencing an index inside this flat storage.</p> <p>This abstraction is mostly invisible in modern PyTorch, but helpful for:</p> <ul> <li> <p>Debugging memory errors</p> </li> <li> <p>Understanding views, slicing, and contiguity</p> </li> <li> <p>Saving custom binary formats</p> </li> </ul>"},{"location":"chapter16_storage_memory/#162-tensor-views-and-storage-sharing","title":"16.2 Tensor Views and Storage Sharing","text":"<p>Tensors created from one another may share storage, meaning changes in one affect the other:</p> <pre><code>a = torch.tensor([1, 2, 3, 4])\nb = a.view(2, 2)\nb[0][0] = 99\nprint(a)  # tensor([99, 2, 3, 4]) \u2014 same storage!\n</code></pre> <p>\u2705 If you want independent memory, use .clone(): <pre><code>c = a.clone()\n</code></pre></p>"},{"location":"chapter16_storage_memory/#163-contiguity-and-is_contiguous","title":"16.3 Contiguity and .is_contiguous()","text":"<p>Contiguous memory layout = row-major (C-style ordering).</p> <pre><code>x = torch.randn(2, 3)\nx_T = x.t()\nprint(x_T.is_contiguous())  # False \u2014 transposing breaks contiguity\n</code></pre>"},{"location":"chapter16_storage_memory/#make-tensor-contiguous-again","title":"\u27a4 Make tensor contiguous again:","text":"<pre><code>x_T_contig = x_T.contiguous()\n</code></pre> <p>\u26a0\ufe0f Functions like .view() only work on contiguous tensors.  \u2705 Use .reshape() as a safer alternative.</p>"},{"location":"chapter16_storage_memory/#164-memory_format-controlling-layout-eg-nhwc-vs-nchw","title":"16.4 memory_format \u2014 Controlling Layout (e.g. NHWC vs NCHW)","text":"<p>In deep learning, layout matters \u2014 especially for performance on GPUs or specialized hardware.</p>"},{"location":"chapter16_storage_memory/#common-formats","title":"Common formats:","text":"<ul> <li> <p>torch.contiguous_format \u2192 NCHW (batch, channel, height, width)</p> </li> <li> <p>orch.channels_last \u2192 NHWC (optimized for conv2d on GPU)</p> </li> </ul>"},{"location":"chapter16_storage_memory/#convert-format","title":"\u27a4 Convert format:","text":"<pre><code>x = torch.randn(1, 3, 224, 224)  # NCHW by default\nx_cl = x.to(memory_format=torch.channels_last)\n</code></pre>"},{"location":"chapter16_storage_memory/#check-format","title":"\u27a4 Check format:","text":"<pre><code>x.is_contiguous(memory_format=torch.channels_last)\n</code></pre> <p>\u2705 channels_last format boosts performance for 2D convolutions on modern GPUs (A100, RTX, etc.)</p>"},{"location":"chapter16_storage_memory/#165-saving-and-loading-tensors","title":"16.5 Saving and Loading Tensors","text":""},{"location":"chapter16_storage_memory/#save-raw-tensor-uses-underlying-storage","title":"\u27a4 Save raw tensor (uses underlying storage):","text":"<pre><code>torch.save(tensor, 'tensor.pt')\ntensor_loaded = torch.load('tensor.pt')\n</code></pre>"},{"location":"chapter16_storage_memory/#write-your-own-binary-format","title":"\u27a4 Write your own binary format:","text":"<pre><code>with open('my_tensor.bin', 'wb') as f:\n    f.write(tensor.numpy().tobytes())\n</code></pre>"},{"location":"chapter16_storage_memory/#load-from-raw-bytes","title":"\u27a4 Load from raw bytes:","text":"<pre><code>tensor_from_bin = torch.frombuffer(open('my_tensor.bin', 'rb').read(), dtype=torch.float32)\n</code></pre> <p>Useful for embedded systems, device-to-device transfer, legacy platforms.</p>"},{"location":"chapter16_storage_memory/#166-when-does-this-matter","title":"16.6 When Does This Matter?","text":"Use Case Why It Matters Custom tensor manipulation Avoid unintended memory sharing Model performance (conv2d) Layout format can impact speed Multi-threading or slicing Views can lead to hidden memory bugs Saving large datasets Storage-level access may be needed Deployment to accelerators Requires channels_last layout"},{"location":"chapter16_storage_memory/#167-summary","title":"\u2705 16.7 Summary","text":"Concept Purpose <code>tensor.storage()</code> Inspect or share memory manually <code>.clone()</code> Create independent memory copy <code>.is_contiguous()</code> Check if memory is sequential <code>.contiguous()</code> Fix layout to use <code>.view()</code> safely <code>memory_format</code> Optimize layout for conv ops (NCHW/NHWC) <ul> <li> <p>Most users won\u2019t need to use <code>.storage()</code> directly \u2014 but for debugging weird behavior, it\u2019s a lifesaver</p> </li> <li> <p>For performance optimization (especially with CNNs), switching to <code>channels_last</code> is one of the easiest wins</p> </li> <li> <p>Memory format awareness becomes essential when building custom ops, exporting to ONNX/TensorRT, or scaling models</p> </li> </ul>"},{"location":"chapter17_cuda/","title":"Chapter 17: Using <code>torch</code> with CUDA","text":"<p>\u201cIf your tensors aren\u2019t on the GPU, are they even lifting?\u201d</p>"},{"location":"chapter17_cuda/#171-what-is-cuda","title":"17.1 What is CUDA?","text":"<p>CUDA stands for Compute Unified Device Architecture \u2014 NVIDIA\u2019s parallel computing platform.</p> <p>In PyTorch, it means:</p> <ul> <li>Massive speedups via GPU acceleration </li> <li>Easy-to-use APIs to move computation to CUDA  </li> <li>Seamless switching between CPU and GPU  </li> </ul> <p>PyTorch abstracts CUDA beautifully. If you can use <code>.to('cuda')</code>, you can GPU.</p>"},{"location":"chapter17_cuda/#172-check-cuda-availability","title":"17.2 Check CUDA Availability","text":"<p>Before using CUDA, always check:</p> <pre><code>import torch\ntorch.cuda.is_available()  # Returns True if CUDA is ready\ntorch.cuda.device_count()  # Number of available GPUs\n</code></pre>"},{"location":"chapter17_cuda/#173-setting-your-device","title":"17.3 Setting Your Device","text":"<pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nx = torch.randn(5, 5).to(device)\nmodel = model.to(device)\n</code></pre> <p>You can also specify GPU index: <code>'cuda:0'</code>, <code>'cuda:1'</code>, etc.</p>"},{"location":"chapter17_cuda/#174-moving-data-to-and-from-gpu","title":"17.4 Moving Data to and from GPU","text":"<pre><code>x = torch.tensor([1.0, 2.0])\nx_cuda = x.to('cuda')\n\n# Back to CPU\nx_cpu = x_cuda.to('cpu')\n</code></pre> <p>\u26a0\ufe0f Tensors must be on the same device for math to work.  \u274c CPU-GPU ops will crash with RuntimeError.</p>"},{"location":"chapter17_cuda/#175-multi-gpu-usage","title":"17.5 Multi-GPU Usage","text":""},{"location":"chapter17_cuda/#list-all-gpus","title":"\u27a4 List all GPUs:","text":"<pre><code>for i in range(torch.cuda.device_count()):\n    print(torch.cuda.get_device_name(i))\n</code></pre>"},{"location":"chapter17_cuda/#move-model-to-a-specific-gpu","title":"\u27a4 Move model to a specific GPU:","text":"<pre><code>model = model.to('cuda:1')\n</code></pre>"},{"location":"chapter17_cuda/#use-dataparallel-basic-multi-gpu-training","title":"\u27a4 Use DataParallel (basic multi-GPU training):","text":"<pre><code>from torch.nn import DataParallel\n\nmodel = DataParallel(model)\nmodel = model.to('cuda')\n</code></pre> <p>\u2705 Automatically splits input batches  For large-scale training, DistributedDataParallel is preferred (coming in advanced chapters)</p>"},{"location":"chapter17_cuda/#176-memory-management-and-stats","title":"17.6 Memory Management and Stats","text":""},{"location":"chapter17_cuda/#track-vram-usage","title":"\u27a4 Track VRAM usage:","text":"<pre><code>torch.cuda.memory_allocated()\ntorch.cuda.memory_reserved()\n</code></pre>"},{"location":"chapter17_cuda/#free-unused-memory","title":"\u27a4 Free unused memory:","text":"<pre><code>torch.cuda.empty_cache()\n</code></pre> <p>This won\u2019t free memory from PyTorch internally, but it makes it available to other applications.</p>"},{"location":"chapter17_cuda/#177-common-cuda-pitfalls","title":"17.7 Common CUDA Pitfalls","text":"Pitfall Fix Mixing CPU and GPU tensors <code>.to(device)</code> both inputs before operations Forgetting <code>.to(device)</code> on model Model stays on CPU \u2192 loss never goes down Out of memory (OOM) Reduce batch size or use with <code>torch.no_grad()</code> CUDA slower than CPU (tiny model) CUDA overhead may outweigh benefits GPU idle, CPU overloaded Use <code>num_workers</code> in DataLoader + pin_memory"},{"location":"chapter17_cuda/#178-amp-automatic-mixed-precision","title":"17.8 AMP (Automatic Mixed Precision)","text":"<p>For faster training with less memory usage: <pre><code>from torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\nfor input, target in dataloader:\n    optimizer.zero_grad()\n    with autocast():\n        output = model(input)\n        loss = loss_fn(output, target)\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n</code></pre></p> <p>AMP = \u26a1 Speed + \ud83d\udcbe Efficiency without major code rewrites.</p>"},{"location":"chapter17_cuda/#179-benchmark-settings-cudnn","title":"17.9 Benchmark Settings (CuDNN)","text":"<p><pre><code>torch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.deterministic = False\n</code></pre> - Set <code>benchmark=True</code> to let PyTorch auto-optimize conv performance</p> <ul> <li>Set <code>deterministic=True</code> if you need exact reproducibility</li> </ul>"},{"location":"chapter17_cuda/#1710-summary","title":"17.10 Summary","text":"Action Code Example Set device device = torch.device(\"cuda\") Move tensor/model .to(device) Multi-GPU (basic) torch.nn.DataParallel(model) Monitor memory usage memory_allocated(), empty_cache() Mixed precision training torch.cuda.amp.autocast() <ul> <li> <p>GPUs = speed \u2014 use them wisely</p> </li> <li> <p>.to(device) is your best friend \u2014 for tensors, models, inputs, and labels</p> </li> <li> <p>Track your memory, use AMP for large models, and never mix CPU + CUDA in a single operation</p> </li> </ul>"},{"location":"chapter18_numpy/","title":"Chapter 18: Integration with NumPy","text":"<p>\u201cTwo libraries. One memory space. No nonsense.\u201d</p>"},{"location":"chapter18_numpy/#181-why-integrate-with-numpy","title":"18.1 Why Integrate with NumPy?","text":"<p>NumPy is the OG of numerical computing in Python. Even if you're deep into PyTorch, you\u2019ll often need to:</p> <ul> <li>Preprocess data with NumPy  </li> <li>Use NumPy functions not in PyTorch  </li> <li>Interface with external libs (e.g., OpenCV, SciPy, Pandas)  </li> <li>Visualize tensors using matplotlib or seaborn  </li> </ul> <p>PyTorch makes this easy by letting you share memory between <code>torch.Tensor</code> and <code>np.ndarray</code>.</p>"},{"location":"chapter18_numpy/#182-torchfrom_numpy-numpy-tensor","title":"18.2 <code>torch.from_numpy()</code> \u2014 NumPy \u279c Tensor","text":"<pre><code>import numpy as np\nimport torch\n\narr = np.array([1.0, 2.0, 3.0])\nt = torch.from_numpy(arr)\n\narr[0] = 99\nprint(t)  # tensor([99., 2., 3.])\n</code></pre> <p>This is fast and efficient \u2014 no memory copy.</p>"},{"location":"chapter18_numpy/#183-numpy-tensor-numpy","title":"18.3 .numpy() \u2014 Tensor \u279c NumPy","text":"<p><pre><code>t = torch.tensor([1.0, 2.0, 3.0])\narr = t.numpy()\n</code></pre> Again, same memory \u2014 not a copy.</p>"},{"location":"chapter18_numpy/#but-you-must-be-on-the-cpu","title":"\ud83d\uded1 But you must be on the CPU:","text":"<pre><code>t = torch.tensor([1.0, 2.0]).to('cuda')\narr = t.cpu().numpy()  # Must move to CPU first!\n</code></pre>"},{"location":"chapter18_numpy/#184-use-case-examples","title":"18.4 Use Case Examples","text":""},{"location":"chapter18_numpy/#matplotlib-visualization","title":"\u27a4 Matplotlib visualization:","text":"<pre><code>import matplotlib.pyplot as plt\nimage = torch.randn(28, 28)\nplt.imshow(image.numpy(), cmap='gray')\n</code></pre>"},{"location":"chapter18_numpy/#pandas-csv-io","title":"\u27a4 Pandas &amp; CSV I/O:","text":"<pre><code>import pandas as pd\ndf = pd.read_csv('data.csv')\ntensor = torch.from_numpy(df.values).float()\n</code></pre>"},{"location":"chapter18_numpy/#feature-extraction-math-ops","title":"\u27a4 Feature extraction / math ops:","text":"<p><pre><code>np.mean(tensor.numpy(), axis=0)\n</code></pre> Use NumPy when you need broadcasting or functions that PyTorch lacks (e.g., np.percentile()).</p>"},{"location":"chapter18_numpy/#185-zero-copy-interoperability","title":"18.5 Zero-Copy Interoperability","text":"<p>The NumPy\u2194Torch conversion is zero-copy. That\u2019s great, but watch for:</p> Gotcha Solution CUDA tensors can't <code>.numpy()</code> Move to CPU first: t.cpu()<code>.numpy()</code> Detached views may be unsafe Use <code>.clone()</code> if unsure Mixed float types (e.g. float64) Use <code>.float()</code> before model usage In-place ops affect both Clone before modifying either"},{"location":"chapter18_numpy/#186-when-not-to-use-numpy","title":"\ud83d\udeab 18.6 When Not to Use <code>.numpy()</code>","text":"<p>Avoid inside:</p> <ul> <li> <p>Training loops \u2014 breaks CUDA pipelines and slows performance</p> </li> <li> <p>Tensors requiring gradients \u2014 <code>.numpy()</code> drops the computation graph</p> </li> <li> <p>GPU batches \u2014 unnecessary overhead</p> </li> </ul> <p>Use <code>.numpy()</code> only for:</p> <ul> <li> <p>Visualization</p> </li> <li> <p>Evaluation</p> </li> <li> <p>Exporting data</p> </li> <li> <p>Debugging</p> </li> </ul>"},{"location":"chapter18_numpy/#187-tips-for-interfacing-with-numpy","title":"18.7 Tips for Interfacing with NumPy","text":"<ul> <li>Use <code>.detach().cpu().numpy()</code> to safely extract predictions:</li> </ul> <pre><code>preds = model(x)\nnp_preds = preds.detach().cpu().numpy()\n</code></pre> <ul> <li> <p>Convert back to <code>float32</code> if NumPy defaults to <code>float64</code>: <pre><code>torch.from_numpy(arr.astype(np.float32))\n</code></pre></p> </li> <li> <p>Use <code>.contiguous()</code> if you hit weird shape bugs: <pre><code>tensor = tensor.permute(1, 0).contiguous()\n</code></pre></p> </li> </ul>"},{"location":"chapter18_numpy/#188-summary","title":"18.8 Summary","text":"Direction Method NumPy \u2192 Tensor <code>torch.from_numpy()</code> Tensor \u2192 NumPy <code>.numpy()</code> Ensure CPU-only <code>.cpu().numpy()</code> Keep gradients safe <code>.detach().cpu().numpy()</code> <ul> <li> <p>PyTorch and NumPy play together beautifully \u2014 just watch out for GPU vs CPU boundaries</p> </li> <li> <p>These conversions are efficient, but must be used with care during training</p> </li> </ul>"},{"location":"chapter19_debugging/","title":"Chapter 19: Debugging, Profiling, and Best Practices","text":"<p>\u201cWhere code either gets smarter\u2026 or gets you fired.\u201d</p> <p>Let\u2019s wrap up Part IV with the good stuff: not the fancy models or sexy math, but the tools that make sure your code doesn\u2019t silently ruin your entire experiment while you\u2019re staring at a loss curve wondering what went wrong.</p> <p>This chapter is your battle-tested field guide for debugging, profiling, and writing PyTorch that doesn\u2019t betray you.</p>"},{"location":"chapter19_debugging/#191-debugging-tensor-values","title":"19.1 Debugging Tensor Values","text":"<p>First rule of PyTorch debugging: Check your tensors early and often.</p> <pre><code>print(tensor.shape)\nprint(torch.isnan(tensor).any())\nprint(torch.isinf(tensor).any())\n</code></pre>"},{"location":"chapter19_debugging/#check-for-explodingvanishing-gradients","title":"\u27a4 Check for exploding/vanishing gradients:","text":"<pre><code>for name, param in model.named_parameters():\n    if param.grad is not None:\n        print(f\"{name}: grad norm = {param.grad.norm()}\")\n</code></pre>"},{"location":"chapter19_debugging/#192-common-silent-killers","title":"19.2 Common Silent Killers","text":"Bug Symptom Fix Using .data Breaks autograd Use <code>.detach()</code> Mixing CPU and CUDA tensors RuntimeError or silent slowdown Use <code>.to(device)</code> consistently Forgetting model.train() Dropout/BatchNorm behaves incorrectly Always use <code>.train()</code> / .<code>eval()</code> Wrong input shapes Model compiles but outputs garbage Print input/output shapes before layers In-place ops Loss gets stuck / None gradients Avoid <code>x += y</code>; use <code>x = x + y</code>"},{"location":"chapter19_debugging/#193-debug-mode-with-torchautogradset_detect_anomaly","title":"19.3 Debug Mode with torch.autograd.set_detect_anomaly","text":"<p>Use this to catch:</p> <ul> <li> <p>In-place ops that break gradients</p> </li> <li> <p>NaNs in backward pass</p> </li> <li> <p>Invalid computation graph paths</p> </li> </ul> <pre><code>with torch.autograd.set_detect_anomaly(True):\n    loss.backward()\n</code></pre> <p>\u26a0\ufe0f Slightly slower \u2014 but worth it during debugging.</p>"},{"location":"chapter19_debugging/#194-profiler-for-performance-tuning","title":"19.4 Profiler for Performance Tuning","text":"<p>Basic usage: <pre><code>with torch.profiler.profile(\n    activities=[\n        torch.profiler.ProfilerActivity.CPU,\n        torch.profiler.ProfilerActivity.CUDA],\n    record_shapes=True,\n    profile_memory=True\n) as prof:\n    run_training_step()\n\nprint(prof.key_averages().table(sort_by=\"cuda_time_total\"))\n</code></pre></p> <p>Shows CPU and GPU time per op \u2014 useful for finding bottlenecks.</p>"},{"location":"chapter19_debugging/#195-tracking-gpu-memory","title":"19.5 Tracking GPU Memory","text":"<p><pre><code>print(torch.cuda.memory_summary(device=None, abbreviated=False))\n</code></pre> Common VRAM hogs:</p> <ul> <li> <p>Large models without checkpoint()</p> </li> <li> <p>Storing intermediate results (forgetting to .detach())</p> </li> <li> <p>Retaining computation graphs across batches</p> </li> </ul>"},{"location":"chapter19_debugging/#196-tips-for-clean-modular-code","title":"19.6 Tips for Clean, Modular Code","text":"<p>Use a consistent device management strategy: <pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nx = x.to(device)\nmodel = model.to(device)\n</code></pre> Structure your project:</p> <ul> <li> <p><code>model.py</code> \u2014 architectures</p> </li> <li> <p><code>train.py</code> \u2014 training loop</p> </li> <li> <p><code>utils.py</code> \u2014 reusable functions</p> </li> <li> <p><code>config.py</code> \u2014 hyperparameters</p> </li> <li> <p><code>debug.py</code> \u2014 sanity checkers, asserts</p> </li> </ul>"},{"location":"chapter19_debugging/#197-sanity-check-checklist","title":"19.7 Sanity Check Checklist","text":"<p>\u2714 Do your model inputs/outputs have expected shapes? \u2714 Are <code>.requires_grad</code> flags correctly set? \u2714 Is your loss decreasing over time? \u2714 Do <code>.grad</code> values explode or vanish? \u2714 Did you call <code>.train()</code> and <code>.eval()</code> properly? \u2714 Are you detaching everything you log or store? \u2714 Are any tensors stuck on CPU while the model is on GPU?</p>"},{"location":"chapter19_debugging/#198-best-practices-at-a-glance","title":"19.8 Best Practices at a Glance","text":"Practice Why it Matters Always zero gradients Avoid accumulation across batches Use <code>.detach()</code> for logging Avoid unwanted graph retention Profile early Find slow layers before deployment Use mixed precision Save memory, speed up training Assert shapes regularly Prevent silent failures Avoid silent overfitting Validate early, not just at the end"},{"location":"chapter19_debugging/#199-summary","title":"19.9 Summary","text":"Tool / Tip Use Case <code>set_detect_anomaly(True)</code> Catch bad gradients / in-place ops <code>torch.profiler</code> Pinpoint slow layers <code>.grad.norm()</code> monitoring Detect exploding/vanishing gradients <code>memory_summary()</code> See where your VRAM is going Code modularization Keeps training and model logic clean <p>PyTorch is flexible \u2014 but that flexibility means you have to be responsible for sanity.  Trust nothing. Print everything. Profile often.</p>"},{"location":"chapter1_what_is_torch/","title":"Chapter 1: What is <code>torch</code> and Why Does It Matter?","text":""},{"location":"chapter1_what_is_torch/#11-welcome-to-the-core-of-pytorch","title":"1.1 Welcome to the Core of PyTorch","text":"<p>At the very heart of PyTorch lies the <code>torch</code> package. If PyTorch were a house, <code>torch</code> would be the concrete foundation, plumbing, and electrical wiring all rolled into one. It's not just a submodule. It's the core engine that powers everything from simple tensors to high-performance matrix operations on GPUs.</p> <p>You\u2019ve likely seen it in action already:</p> <pre><code>import torch\nx = torch.tensor([1.0, 2.0, 3.0])\n</code></pre> <p>Simple, right? But under the hood, that single line taps into one of the most powerful numerical computing backends available in Python today. This chapter sets the tone for the entire journey ahead. We\u2019ll unpack the purpose of <code>torch</code>, how it fits into the broader PyTorch ecosystem, and why it\u2019s such a critical layer for building everything from linear regressors to generative adversarial networks (GANs).</p>"},{"location":"chapter1_what_is_torch/#12-the-role-of-torch-in-pytorch","title":"1.2 The Role of torch in <code>PyTorch</code>","text":"<p>Let\u2019s clarify something first: PyTorch is not one monolithic library \u2014 it\u2019s a carefully layered system. Think of it like a software lasagna: - <code>torch</code> is the low-level API, providing raw tensor operations, memory management, and mathematical building blocks.  - <code>torch.nn</code> builds on top of <code>torch</code>, adding neural network abstractions.  - <code>torchvision</code>, <code>torchaudio</code>, <code>torchtext</code> are domain-specific wrappers that leverage the torch core for real-world data.  - <code>torch.autograd</code> provides automatic differentiation by hooking into <code>torch.Tensor</code>. </p> <p>If you strip everything else away, you can still build a neural network from scratch using just <code>torch</code>. That\u2019s the level of power and granularity it offers.</p>"},{"location":"chapter1_what_is_torch/#13-why-you-should-care-about-torch","title":"1.3 Why You Should Care About <code>torch</code>","text":"<p>If you want to: - Write custom layers - Debug tensor shape mismatches - Optimize models for edge devices - Dive into research with experimental architectures</p> <p>...then understanding <code>torch</code> is non-negotiable. The higher-level abstractions are fantastic \u2014 until they aren\u2019t. Sooner or later, you\u2019ll find yourself debugging raw tensors, writing custom operations, or squeezing every ounce of performance out of GPU memory. That\u2019s where <code>torch</code> becomes your best friend. Learning <code>torch</code> means: - You control what happens. - You debug faster. - You innovate beyond plug-and-play libraries.</p>"},{"location":"chapter1_what_is_torch/#14-the-tensor-is-everything","title":"1.4 The Tensor is Everything","text":"<p>In <code>torch</code>, everything begins with the <code>Tensor</code> object. If you're from the NumPy world, think of it as NumPy's cooler sibling \u2014 but with superpowers like GPU acceleration, autograd compatibility, and deep learning optimization.</p> <pre><code>x = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\nprint(x.shape)  # torch.Size([2, 2])\n</code></pre> <p>Tensors in <code>torch</code> are not just data containers. They\u2019re the fundamental unit of computation. That means all training data, weights, gradients, and intermediate activations \u2014 they\u2019re all tensors. And every API in PyTorch that does math, manipulation, or optimization? It starts with <code>torch.Tensor</code>. In the next chapter, we\u2019ll fully dissect <code>torch.Tensor</code> and show you just how much power lies in the humble variable <code>x</code>.</p>"},{"location":"chapter1_what_is_torch/#15-summary","title":"1.5 Summary","text":"<ul> <li> <p><code>torch</code> is the foundational library of PyTorch.</p> </li> <li> <p>It handles tensors, math operations, device management, and more.</p> </li> <li> <p>Understanding <code>torch</code> gives you low-level control and high-level mastery.</p> </li> <li> <p>Everything in PyTorch builds on top of <code>torch.Tensor</code>.</p> </li> </ul> <p>Whether you\u2019re training an LSTM on Shakespeare or building a diffusion model for image generation \u2014 <code>torch</code> is the engine under the hood. So buckle up, we\u2019re diving deep.</p>"},{"location":"chapter2_installation_setup/","title":"Chapter 2: Installation &amp; Setup","text":"<p>\u201cA neural net\u2019s journey begins with a single tensor.\u201d</p>"},{"location":"chapter2_installation_setup/#21-installing-pytorch-the-right-way","title":"2.1 Installing PyTorch the Right Way","text":""},{"location":"chapter2_installation_setup/#step-1-visit-the-official-installer-page","title":"\u2705 Step 1: Visit the Official Installer Page","text":"<p>Go to: https://pytorch.org/get-started/locally</p> <p>You\u2019ll see a selector for:</p> <ul> <li>PyTorch Build (Stable / Preview)</li> <li>Your OS (Linux, Mac, Windows)</li> <li>Package Manager (<code>pip</code>, <code>conda</code>)</li> <li>Language (<code>Python</code>, <code>C++</code>)</li> <li>Compute Platform (<code>CPU</code>, <code>CUDA 11.8</code>, <code>CUDA 12.x</code>, etc.)</li> </ul>"},{"location":"chapter2_installation_setup/#recommendation","title":"Recommendation","text":"<p>For most ML developers (especially if you\u2019re using a GPU):</p> <p><pre><code># For pip + CUDA 11.8\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n</code></pre> If you don\u2019t have a GPU, install CPU-only:</p> <pre><code>pip3 install torch torchvision torchaudio\n</code></pre> <p>\u26a0\ufe0f Heads-up: Make sure your Python version is 3.8\u20133.12. PyTorch isn\u2019t too happy with older versions.</p>"},{"location":"chapter2_installation_setup/#22-virtual-environment-setup-highly-recommended","title":"2.2 Virtual Environment Setup (Highly Recommended)","text":"<p>To avoid breaking other Python projects: <pre><code>python -m venv torch_env\nsource torch_env/bin/activate  # On Windows: torch_env\\Scripts\\activate\n</code></pre> Then install PyTorch inside that environment.</p>"},{"location":"chapter2_installation_setup/#23-verify-the-installation","title":"2.3 Verify the Installation","text":"<p>Let\u2019s test it right away: <pre><code>import torch\nx = torch.rand(3, 3)\nprint(\"Tensor:\\n\", x)\nprint(\"Is CUDA available?\", torch.cuda.is_available())\n</code></pre> You should see a 3\u00d73 matrix of random values and a True/False flag about CUDA.</p>"},{"location":"chapter2_installation_setup/#24-your-first-tensor-playground","title":"2.4 Your First Tensor Playground","text":"<p>Let\u2019s walk through a few core operations to prove it works \u2014 and start building intuition.</p> <p>\u27a4 Create a Tensor <pre><code>a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n</code></pre> \u27a4 Do Some Math <pre><code>b = torch.ones_like(a)\nc = a + b\nprint(\"Addition:\\n\", c)\n</code></pre> \u27a4 Matrix Multiplication <pre><code>d = torch.matmul(a, c.T)  # Transpose and multiply\nprint(\"Matrix product:\\n\", d)\n</code></pre> \u27a4 Move to GPU (if available) <pre><code>if torch.cuda.is_available():\n    a = a.to(\"cuda\")\n    print(\"Tensor on GPU:\", a)\n</code></pre></p>"},{"location":"chapter2_installation_setup/#25-cpu-vs-cuda-why-it-matters","title":"2.5 CPU vs CUDA: Why It Matters","text":"Operation CPU CUDA (GPU) Matrix multiplication Slower for large matrices Highly optimized Memory access Direct system memory VRAM on GPU Use-case Lightweight ML / debugging Training large models <p>TL;DR: Use CUDA if available. It\u2019s fast. Like, ridiculously fast.</p>"},{"location":"chapter2_installation_setup/#26-troubleshooting-tips","title":"2.6 Troubleshooting Tips","text":"<p>\u274c \u201ctorch not found\u201d Make sure:</p> <ul> <li> <p>You activated the correct environment</p> </li> <li> <p>You installed it in the right Python version</p> </li> </ul> <p>\u274c CUDA not available  - Check your driver: <pre><code>nvidia-smi\n</code></pre></p> <p>Make sure your CUDA toolkit matches the version you selected during install.</p>"},{"location":"chapter2_installation_setup/#27-quick-recap","title":"2.7 Quick Recap","text":"<p>By now, you should:</p> <p>\u2705 Have PyTorch installed in a clean environment</p> <p>\u2705 Know how to write and run basic tensor operations</p> <p>\u2705 Understand the role of CUDA and how to check if it\u2019s working</p>"},{"location":"chapter3_tensor_fundamentals/","title":"Chapter 3: Tensor Fundamentals","text":"<p>\u201cBefore we reshape the world with tensors, let\u2019s understand what they truly are.\u201d</p>"},{"location":"chapter3_tensor_fundamentals/#31-what-is-a-tensor","title":"\ud83d\udd0d 3.1 What Is a Tensor?","text":"<p>A tensor is the core data structure in PyTorch. It\u2019s a multi-dimensional array that represents data \u2014 and supports a wide range of mathematical operations on CPUs or GPUs.</p> <p>But that definition alone is too dry. Let\u2019s break it down by dimensions:</p> Tensor Dim Common Name Example Shape 0D Scalar <code>42</code> <code>()</code> 1D Vector <code>[1, 2, 3]</code> <code>(3,)</code> 2D Matrix <code>[[1, 2], [3, 4]]</code> <code>(2, 2)</code> 3D+ Tensor (generic) Batch of images, video <code>(B, C, H, W)</code> <p>In PyTorch:</p> <p><pre><code>import torch\nx = torch.tensor([[1, 2], [3, 4]])\n</code></pre> This creates a 2\u00d72 matrix \u2014 a rank-2 tensor. All machine learning models, no matter how complex, are just functions that operate on tensors.</p>"},{"location":"chapter3_tensor_fundamentals/#32-tensors-vs-numpy-arrays","title":"3.2 Tensors vs NumPy Arrays","text":"Feature NumPy<code>ndarray</code> PyTorch<code>Tensor</code> GPU support \u274c CPU only \u2705 CUDA, MPS, etc. Autograd \u274c No automatic gradients \u2705 Built-in autograd Deep learning ready \u274c Needs integration \u2705 Native support API similarity \u2705 High \u2705 Nearly identical Interoperability \u2705 torch.from_numpy() / .numpy() <p>PyTorch was designed to mimic NumPy\u2019s API \u2014 so if you\u2019ve written code in NumPy before, tensors will feel familiar. But they also bring GPU acceleration and automatic differentiation into the mix. <pre><code>import numpy as np\nnp_arr = np.array([1.0, 2.0, 3.0])\ntorch_tensor = torch.from_numpy(np_arr)\n# Back to NumPy\nnp_back = torch_tensor.numpy()\n</code></pre></p> <p>\u26a0\ufe0f Note: Both share the same memory buffer unless you .clone() the tensor.</p>"},{"location":"chapter3_tensor_fundamentals/#33-device-management-cpu-and-cuda","title":"3.3 Device Management: CPU and CUDA","text":"<p>By default, PyTorch tensors live on the CPU. But the magic happens when you move them to the GPU (via CUDA). Check if CUDA is available: <pre><code>torch.cuda.is_available()  # Returns True if you have a compatible GPU\n</code></pre> Move tensor to CUDA: <pre><code>x = torch.tensor([1.0, 2.0])\nx = x.to('cuda')        # Send to GPU\nx = x.to('cpu')         # Bring back to CPU\n</code></pre> You can also be explicit: <pre><code>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nx = torch.tensor([1.0, 2.0], device=device)\n</code></pre></p>"},{"location":"chapter3_tensor_fundamentals/#34-cpu-vs-gpu-when-it-matters","title":"3.4 CPU vs GPU: When It Matters","text":"Use Case CPU GPU Small models or inference \u2705 Fine \u2705 Maybe overkill Deep neural nets \u274c Too slow \u2705 Ideal Training loops w/ backprop \u274c Bottleneck \u2705 Accelerated Parallel data ops \u2705 w/ multiprocessing \u2705 Massively parallel <p>\ud83d\udca1 Even simple tensor math is often 10\u2013100\u00d7 faster on the GPU.</p>"},{"location":"chapter3_tensor_fundamentals/#35-summary","title":"3.5 Summary","text":"<ul> <li> <p>A tensor is a generalization of scalars, vectors, and matrices.</p> </li> <li> <p>PyTorch\u2019s Tensor is like NumPy\u2019s ndarray, but with GPU and autograd support.</p> </li> <li> <p>Devices (cpu or cuda) matter \u2014 especially for training speed.</p> </li> <li> <p>Tensor creation is simple \u2014 but choosing the right device is crucial for performance.</p> </li> </ul>"},{"location":"chapter4_tensor/","title":"Chapter 4: <code>torch.Tensor</code>","text":"<p>\u201cThe universe of PyTorch begins with a single tensor\u2014and everything else builds on top of it.\u201d</p>"},{"location":"chapter4_tensor/#41-tensor-creation-methods","title":"4.1 Tensor Creation Methods","text":"<p>Let\u2019s explore how to instantiate tensors like a boss. PyTorch provides multiple ways depending on your use case:</p>"},{"location":"chapter4_tensor/#basic-constructor","title":"\u25b6 Basic Constructor","text":"<pre><code>torch.tensor([1, 2, 3])                 # From Python list\ntorch.tensor([[1.0, 2.0], [3.0, 4.0]])  # 2D Float Tensor\n</code></pre>"},{"location":"chapter4_tensor/#pre-filled-factories","title":"\u25b6 Pre-filled Factories","text":"<pre><code>torch.zeros(2, 3)           # All zeros\ntorch.ones(2, 3)            # All ones\ntorch.full((2, 2), 42)      # All elements are 42\ntorch.eye(3)                # Identity matrix\ntorch.arange(0, 10, 2)      # Like Python\u2019s range()\ntorch.linspace(0, 1, 5)     # 5 values between 0 and 1\n</code></pre>"},{"location":"chapter4_tensor/#like-another-tensor","title":"\u25b6 Like Another Tensor","text":"<pre><code>x = torch.ones(2, 2)\ntorch.zeros_like(x)\ntorch.rand_like(x)\n</code></pre>"},{"location":"chapter4_tensor/#42-tensor-properties","title":"\ud83d\udcd0 4.2 Tensor Properties","text":"<p>Every tensor has a few critical attributes: \ud83d\udd38 shape and size() <pre><code>x.shape            # torch.Size([2, 3])\nx.size()           # Same as above\n</code></pre> \ud83d\udd38 dtype \u2013 data type <pre><code>x.dtype            # e.g., torch.float32\nx = x.to(torch.int64)  # change type\n</code></pre> \ud83d\udd38 device \u2013 CPU or CUDA <pre><code>x.device           # Shows current device\nx = x.to('cuda')   # Move to GPU\n</code></pre> \ud83d\udd38 requires_grad <pre><code>x = torch.tensor([1.0, 2.0], requires_grad=True)\nx.requires_grad    # True\n</code></pre></p>"},{"location":"chapter4_tensor/#43-tensor-operations","title":"4.3 Tensor Operations","text":"<p>PyTorch supports extensive elementwise operations directly on tensors:</p> <p>Arithmetic <pre><code>x + y\nx - y\nx * y\nx / y\nx ** 2\n</code></pre> Comparison <pre><code>x == y\nx &gt; y\nx != y\n</code></pre> Logical <pre><code>torch.logical_and(x &gt; 0, x &lt; 1)\n</code></pre></p> <p>\u2705 These are vectorized \u2014 no need for loops!</p>"},{"location":"chapter4_tensor/#44-reshaping-reorganizing-tensors","title":"4.4 Reshaping &amp; Reorganizing Tensors","text":"<p>These tools let you morph tensor shapes without changing their content.</p> <p>Reshape &amp; View <pre><code>x.view(-1)           # Flatten (requires contiguous memory)\nx.reshape(2, 3)      # Flexible reshape\n</code></pre> Squeeze &amp; Unsqueeze <pre><code>x = torch.zeros(1, 3, 1)\nx.squeeze()          # Remove dim=1 \u2192 (3,)\nx.unsqueeze(0)       # Add dim at index 0 \u2192 (1, 3, 1)\n</code></pre> Permute &amp; Transpose <pre><code>x = torch.randn(2, 3, 4)\nx.permute(2, 0, 1)   # Reorders dimensions\nx.transpose(0, 1)    # Swaps two dims only\n</code></pre></p> <p>\ud83d\udd01 Use permute() for high-dimensional tensors (images, etc.)</p>"},{"location":"chapter4_tensor/#45-indexing-slicing","title":"4.5 Indexing &amp; Slicing","text":"<p>Basic and advanced ways to access tensor values: <pre><code>x[0]             # First row\nx[:, 1]          # Second column\nx[1:, :]         # All rows except the first\n</code></pre> Also supports: <pre><code>x[x &gt; 0]                         # Boolean mask\nx[torch.tensor([0, 2])]          # Indexing with tensor\n</code></pre></p> <p>These are the same ideas as NumPy \u2014 but with GPU support.</p>"},{"location":"chapter4_tensor/#46-autograd-compatibility","title":"4.6 Autograd Compatibility","text":"<p>One of PyTorch\u2019s killer features is automatic differentiation \u2014 made possible because every Tensor can carry its computation history. <pre><code>x = torch.tensor([2.0, 3.0], requires_grad=True)\ny = x ** 2 + 3\nz = y.sum()\nz.backward()\nx.grad  # \u2202z/\u2202x\n</code></pre> Don\u2019t track: <pre><code>with torch.no_grad():\n    result = model(x)\n</code></pre></p> <p>\ud83d\udd25 If requires_grad=True, the tensor is part of the computation graph. Perfect for training neural nets.</p>"},{"location":"chapter4_tensor/#47-miscellaneous-api-tricks","title":"4.7 Miscellaneous API Tricks","text":"<p>Clone vs Detach: <pre><code>x.clone()        # Returns a copy\nx.detach()       # Returns a tensor with no grad-tracking\n</code></pre> Checking if a tensor is contiguous: <pre><code>x.is_contiguous()\n</code></pre></p> <p>Useful when using .view() which demands contiguous memory layout.</p>"},{"location":"chapter4_tensor/#48-summary","title":"4.8 Summary","text":"<ul> <li> <p><code>torch.Tensor</code> is more than just an array \u2014 it has memory, gradient, and device awareness.</p> </li> <li> <p>You can create tensors using many constructors (<code>zeros</code>, <code>arange</code>, <code>full</code>, etc.)</p> </li> <li> <p>Tensors support rich operations: math, reshape, slice, compare, move to CUDA.</p> </li> <li> <p>They're also autograd-aware \u2014 making them perfect for deep learning.</p> </li> </ul>"},{"location":"chapter5_dtype_device/","title":"Chapter 5: Data Types and Devices","text":"<p>\u201cPrecision and placement \u2014 the twin pillars of efficient tensor computation.\u201d</p>"},{"location":"chapter5_dtype_device/#51-torchdtype-the-soul-of-a-tensor","title":"\ud83e\uddec 5.1 <code>torch.dtype</code>: The Soul of a Tensor","text":"<p>Every tensor in PyTorch carries a data type that defines the precision and nature of its values.</p>"},{"location":"chapter5_dtype_device/#common-torchdtype-values","title":"Common <code>torch.dtype</code> values:","text":"<code>dtype</code> Description Bits <code>torch.float32</code> 32-bit floating point (default) 32 <code>torch.float64</code> 64-bit float (aka double) 64 <code>torch.float16</code> 16-bit float (half precision) 16 <code>torch.bfloat16</code> Brain float (used in TPUs) 16 <code>torch.int32</code> 32-bit integer 32 <code>torch.int64</code> 64-bit integer (aka long) 64 <code>torch.bool</code> Boolean 1"},{"location":"chapter5_dtype_device/#how-to-specify-dtype","title":"How to specify <code>dtype</code>:","text":"<pre><code>x = torch.tensor([1, 2, 3], dtype=torch.float64)\nprint(x.dtype)  # torch.float64\n</code></pre>"},{"location":"chapter5_dtype_device/#casting-between-types","title":"Casting between types:","text":"<pre><code>x = x.to(torch.float16)\nx = x.int()              # Shortcut for int32\nx = x.type(torch.float32)\n</code></pre> <p>\ud83d\udd2c float32 is the sweet spot for training: fast and accurate. But for inference? float16 (or bfloat16) is often enough.</p>"},{"location":"chapter5_dtype_device/#52-torchdevice-the-tensors-location","title":"5.2 <code>torch.device</code>: The Tensor's Location","text":"<p>A tensor doesn\u2019t just live in memory \u2014 it lives on a device. <pre><code>cpu_tensor = torch.tensor([1.0])           # On CPU\ngpu_tensor = cpu_tensor.to('cuda')         # Moved to GPU\n</code></pre></p> <p>You can also create tensors directly on a device: <pre><code>device = torch.device('cuda')\nx = torch.zeros(3, 3, device=device)\n</code></pre></p> <p>Detecting and using available GPUs: <pre><code>if torch.cuda.is_available():\n    print(\"CUDA ready! Let's party.\")\nelse:\n    print(\"Stuck on CPU. Meh.\")\n</code></pre></p> <p>\ud83d\udca1 For multi-GPU systems: use 'cuda:0', 'cuda:1', etc.</p>"},{"location":"chapter5_dtype_device/#53-default-data-type-settings","title":"5.3 Default Data Type Settings","text":"<p>Sometimes you want to globally change the default dtype. PyTorch lets you do this: <pre><code>torch.set_default_dtype(torch.float64)\n</code></pre> This affects all future float tensors created via: <pre><code>torch.zeros(3)      # Now float64\ntorch.tensor([1.0]) # Now float64\n</code></pre> Check the current default: <pre><code>torch.get_default_dtype()\n</code></pre></p> <p>Useful when training scientific models (need precision) or optimizing inference (want float16).</p>"},{"location":"chapter5_dtype_device/#54-intro-to-mixed-precision","title":"5.4 Intro to Mixed Precision","text":"<p>In modern deep learning (especially with GPUs like RTX, A100, etc.), mixed precision is the name of the game.</p>"},{"location":"chapter5_dtype_device/#whats-mixed-precision","title":"What\u2019s Mixed Precision?","text":"<p>Training with both:</p> <ul> <li> <p><code>float32</code> (for critical values like loss gradients)</p> </li> <li> <p><code>float16</code> or <code>bfloat16</code> (for speed and memory savings)</p> </li> </ul>"},{"location":"chapter5_dtype_device/#why-use-it","title":"Why use it?","text":"<ul> <li> <p>\ud83d\ude84 Faster training with Tensor Cores</p> </li> <li> <p>\ud83d\udcc9 Less memory usage = bigger models</p> </li> </ul>"},{"location":"chapter5_dtype_device/#how-to-use-it","title":"How to use it?","text":"<p>Start with PyTorch\u2019s AMP (Automatic Mixed Precision):</p> <pre><code>from torch.cuda.amp import autocast\n\nwith autocast():\n    output = model(input)\n    loss = criterion(output, target)\n</code></pre> <p>\u26a0\ufe0f We\u2019ll cover this fully in Chapter 17: Using Torch with CUDA. For now, just know it\u2019s a killer optimization strategy.</p>"},{"location":"chapter5_dtype_device/#55-summary","title":"5.5 Summary","text":"<ul> <li> <p><code>torch.dtype</code> controls a tensor\u2019s precision and data interpretation.</p> </li> <li> <p><code>torch.device</code> decides where the tensor lives \u2014 CPU or GPU.</p> </li> <li> <p>You can set default dtypes, move tensors across devices, and leverage mixed precision for high-performance computing.</p> </li> <li> <p>These two concepts are subtle but powerful when optimizing both training and inference.</p> </li> </ul>"},{"location":"chapter6_random_seed/","title":"Chapter 6: Random Sampling and Seeding","text":"<p>\u201cTo master the chaos, you must first control the dice.\u201d</p>"},{"location":"chapter6_random_seed/#61-why-randomness-matters-in-deep-learning","title":"\ud83c\udfb2 6.1 Why Randomness Matters in Deep Learning","text":"<p>Randomness shows up everywhere in machine learning:</p> <ul> <li>Weight initialization  </li> <li>Dropout layers  </li> <li>Data shuffling  </li> <li>Augmentation  </li> <li>Mini-batch selection  </li> <li>Sampling in generative models (e.g., GANs)</li> </ul> <p>And PyTorch gives you robust control over all of it.</p>"},{"location":"chapter6_random_seed/#62-common-random-tensor-generators","title":"6.2 Common Random Tensor Generators","text":"<p>PyTorch provides several methods to generate random numbers:</p>"},{"location":"chapter6_random_seed/#torchrandsizes","title":"\u27a4 <code>torch.rand(*sizes)</code>","text":"<p>Returns values \u2208 [0, 1), uniform distribution.</p> <pre><code>torch.rand(2, 3)\n</code></pre>"},{"location":"chapter6_random_seed/#torchrandnsizes","title":"\u27a4 <code>torch.randn(*sizes)</code>","text":"<p>Standard normal distribution (mean=0, std=1) <pre><code>torch.randn(3, 3)\n</code></pre></p>"},{"location":"chapter6_random_seed/#torchrandintlow-high-size","title":"\u27a4 <code>torch.randint(low, high, size)</code>","text":"<p>Uniform integer values from low (inclusive) to high (exclusive) <pre><code>torch.randint(0, 10, (2, 2))\n</code></pre></p>"},{"location":"chapter6_random_seed/#torchrandpermn","title":"\u27a4 <code>torch.randperm(n)</code>","text":"<p>Random permutation of integers from 0 to n-1 <pre><code>torch.randperm(5)  # e.g., tensor([3, 0, 4, 2, 1])\n</code></pre></p>"},{"location":"chapter6_random_seed/#torchbernoulliprobs","title":"\u27a4 <code>torch.bernoulli(probs)</code>","text":"<p>Samples 0 or 1 based on given probabilities (used for dropout) <pre><code>probs = torch.tensor([0.1, 0.5, 0.9])\ntorch.bernoulli(probs)\n</code></pre></p>"},{"location":"chapter6_random_seed/#63-how-to-set-seeds-for-reproducibility","title":"6.3 How to Set Seeds for Reproducibility","text":"<p>Randomness is useful \u2014 until you need your results to be repeatable.</p>"},{"location":"chapter6_random_seed/#global-seed-setter","title":"Global seed setter:","text":"<p><pre><code>torch.manual_seed(42)\n</code></pre> This affects:</p> <ul> <li> <p><code>torch.rand</code>, <code>torch.randn</code>, <code>torch.randint</code></p> </li> <li> <p>Initial weights in models</p> </li> <li> <p>Dropout patterns</p> </li> </ul> <p>If you're using CUDA: <pre><code>torch.cuda.manual_seed(42)\ntorch.cuda.manual_seed_all(42)  # For multi-GPU\n</code></pre></p>"},{"location":"chapter6_random_seed/#64-torchgenerator-when-you-want-control-per-operation","title":"6.4 <code>torch.Generator</code> \u2013 When You Want Control Per Operation","text":"<p>Sometimes you want repeatable randomness without resetting the global seed. That\u2019s where <code>torch.Generator</code> comes in.</p>"},{"location":"chapter6_random_seed/#example","title":"Example:","text":"<p><pre><code>g = torch.Generator()\ng.manual_seed(123)\nx1 = torch.rand(2, 2, generator=g)\nx2 = torch.rand(2, 2, generator=g)  # Continues from the same stream\n</code></pre> This is especially useful when:</p> <ul> <li> <p>You\u2019re writing tests</p> </li> <li> <p>You want independent RNG streams</p> </li> <li> <p>You\u2019re managing parallel data loading or multiprocessing</p> </li> </ul>"},{"location":"chapter6_random_seed/#65-beware-randomness-in-parallel-training","title":"6.5 Beware: Randomness in Parallel Training","text":"<p>If you\u2019re training across:</p> <ul> <li> <p>Multiple GPUs</p> </li> <li> <p>Multiple processes (e.g., <code>DataLoader</code> with <code>num_workers &gt; 0</code>)</p> </li> <li> <p>Multiple epochs with shuffling</p> </li> </ul> <p>Then you must control randomness carefully or you\u2019ll get:</p> <ul> <li> <p>Nondeterministic results</p> </li> <li> <p>Flaky training behavior</p> </li> <li> <p>Inconsistent evaluation metrics</p> </li> </ul>"},{"location":"chapter6_random_seed/#to-mitigate-this","title":"To mitigate this:","text":"<p><pre><code>torch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n</code></pre> And in <code>DataLoader:</code> <pre><code>DataLoader(..., worker_init_fn=seed_worker)\n</code></pre> Where <code>seed_worker</code> reseeds per worker.</p>"},{"location":"chapter6_random_seed/#66-summary","title":"6.6 Summary","text":"<ul> <li> <p>PyTorch offers <code>rand</code>, <code>randn</code>, <code>randint</code>, and <code>randperm</code> for common randomness.</p> </li> <li> <p>Use <code>torch.manual_seed()</code> to control global RNG for reproducibility.</p> </li> <li> <p>Use <code>torch.Generator</code> for isolated, repeatable randomness.</p> </li> <li> <p>Beware: parallelism and dropout can make reproducibility tricky without proper seeding.</p> </li> </ul>"},{"location":"chapter7_math_ops/","title":"Chapter 7: Math Operations","text":"<p>\u201cGive me a tensor and a math op, and I will move the machine learning world.\u201d</p>"},{"location":"chapter7_math_ops/#71-categories-of-math-operations-in-pytorch","title":"7.1 Categories of Math Operations in PyTorch","text":"<p>Math in PyTorch is modular and fast. Most tensor operations fall into one of these:</p> Category Examples Description Elementwise Ops <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>exp</code>, <code>log</code> Operate on each element independently Reduction Ops <code>sum</code>, <code>mean</code>, <code>max</code>, <code>prod</code> Reduce one or more dimensions Matrix Ops <code>matmul</code>, <code>mm</code>, <code>bmm</code>, <code>einsum</code> Tensor/matrix multiplication and contractions Special Ops <code>clamp</code>, <code>abs</code>, <code>floor</code>, <code>round</code> Nonlinear math tricks <p>Let\u2019s break these down with practical examples.</p>"},{"location":"chapter7_math_ops/#72-elementwise-operations","title":"7.2 Elementwise Operations","text":""},{"location":"chapter7_math_ops/#basic-arithmetic","title":"Basic arithmetic:","text":"<pre><code>a = torch.tensor([1.0, 2.0, 3.0])\nb = torch.tensor([3.0, 2.0, 1.0])\na + b\na - b\na * b\na / b\na ** 2\n</code></pre>"},{"location":"chapter7_math_ops/#elementwise-functions","title":"Elementwise functions:","text":"<pre><code>torch.exp(a)\ntorch.log(a)\ntorch.sqrt(a)\ntorch.sin(a)\ntorch.abs(torch.tensor([-3.0, 2.0]))\n</code></pre>"},{"location":"chapter7_math_ops/#in-place-versions","title":"In-place versions:","text":"<pre><code>a.add_(1)  # modifies a directly\n</code></pre> <p>\u26a0\ufe0f Use in-place ops (_) with caution in training \u2014 they can interfere with autograd.</p>"},{"location":"chapter7_math_ops/#73-reduction-operations","title":"7.3 Reduction Operations","text":"<p>Reductions collapse tensor dimensions into a summary value. <pre><code>x = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\ntorch.sum(x)            # Sum of all elements\ntorch.sum(x, dim=0)     # Column-wise sum: tensor([4., 6.])\ntorch.mean(x)           # Mean\ntorch.prod(x)           # Product\ntorch.max(x), torch.min(x)\ntorch.argmax(x), torch.argmin(x)\n</code></pre></p> <p>You can reduce across specific dimensions with the dim= keyword. This is critical for understanding batch-wise behavior in neural networks.</p>"},{"location":"chapter7_math_ops/#74-logical-and-comparison-operations","title":"7.4 Logical and Comparison Operations","text":"<pre><code>a = torch.tensor([1.0, 2.0, 3.0])\nb = torch.tensor([2.0, 2.0, 2.0])\na == b\na &gt; b\na &lt;= b\na != b\n# Use result as mask:\na[a &gt; 2]      # tensor([3.0])\n</code></pre>"},{"location":"chapter7_math_ops/#pytorch-also-supports","title":"PyTorch also supports:","text":"<pre><code>torch.any(condition)\ntorch.all(condition)\n</code></pre>"},{"location":"chapter7_math_ops/#75-matrix-operations-linear-algebra-101","title":"7.5 Matrix Operations (Linear Algebra 101)","text":""},{"location":"chapter7_math_ops/#dot-product-1d","title":"Dot Product (1D):","text":"<pre><code>a = torch.tensor([1.0, 2.0])\nb = torch.tensor([3.0, 4.0])\ntorch.dot(a, b)      # 1*3 + 2*4 = 11\n</code></pre>"},{"location":"chapter7_math_ops/#matrix-multiplication","title":"Matrix Multiplication:","text":"<pre><code>A = torch.tensor([[1, 2], [3, 4]])\nB = torch.tensor([[5, 6], [7, 8]])\nA @ B                # or torch.matmul(A, B)\n</code></pre>"},{"location":"chapter7_math_ops/#batched-multiplication","title":"Batched Multiplication:","text":"<pre><code>A = torch.randn(10, 3, 4)\nB = torch.randn(10, 4, 5)\ntorch.bmm(A, B)      # Batch matrix multiply\n</code></pre>"},{"location":"chapter7_math_ops/#76-einsum-einstein-notation","title":"7.6 einsum: Einstein Notation","text":"<p>A flexible and expressive way to do tensor contractions, transpositions, reductions. <pre><code>A = torch.randn(2, 3)\nB = torch.randn(3, 4)\ntorch.einsum('ik,kj-&gt;ij', A, B)  # Equivalent to matmul\n</code></pre></p> <p>More readable, chainable, and often better for performance in attention mechanisms and complex models.</p>"},{"location":"chapter7_math_ops/#77-special-math-ops","title":"7.7 Special Math Ops","text":""},{"location":"chapter7_math_ops/#clamping-limit-min-and-max","title":"Clamping (limit min and max):","text":"<pre><code>x = torch.tensor([0.1, 0.5, 1.5, 3.0])\ntorch.clamp(x, min=0.2, max=1.0)\n</code></pre>"},{"location":"chapter7_math_ops/#rounding","title":"Rounding:","text":"<pre><code>torch.floor(x)\ntorch.ceil(x)\ntorch.round(x)\n</code></pre>"},{"location":"chapter7_math_ops/#normalization-common-in-ml","title":"Normalization (common in ML):","text":"<pre><code>x = torch.tensor([1.0, 2.0, 3.0])\nx_norm = (x - x.mean()) / x.std()\n</code></pre>"},{"location":"chapter7_math_ops/#78-performance-tips","title":"7.8 Performance Tips","text":"<ul> <li> <p>\u2705 Prefer <code>@</code> or <code>matmul()</code> for clarity and speed.</p> </li> <li> <p>\u2705 Avoid Python for loops over tensor elements \u2014 use broadcasting.</p> </li> <li> <p>\u2705 Use <code>.float()</code> or <code>.half()</code> wisely \u2014 lower precision = faster compute.</p> </li> <li> <p>\u26a0\ufe0f Avoid in-place ops if you're unsure about autograd compatibility.</p> </li> </ul>"},{"location":"chapter7_math_ops/#79-summary","title":"7.9 Summary","text":"Type Examples Elementwise <code>+</code>, <code>*</code>, <code>exp</code>, <code>log</code> Reduction <code>sum</code>, <code>mean</code>, <code>max</code>, <code>prod</code> Matrix <code>@</code>, <code>matmul</code>, <code>bmm</code>, <code>einsum</code> Special Ops <code>clamp</code>, <code>abs</code>, <code>floor</code>, <code>round</code> <ul> <li> <p>PyTorch supports high-performance math via native tensor ops.</p> </li> <li> <p>Knowing when to reduce, broadcast, or <code>matmul</code> is key to writing efficient models.</p> </li> <li> <p>If you're writing anything involving gradients, check for safe usage with autograd.</p> </li> </ul>"},{"location":"chapter8_broadcast_shape/","title":"Chapter 8: Broadcasting and Shape Ops","text":"<p>\u201cShape your tensors, or they will shape your debugging sessions.\u201d</p>"},{"location":"chapter8_broadcast_shape/#81-what-is-broadcasting","title":"8.1 What is Broadcasting?","text":"<p>Broadcasting lets PyTorch perform arithmetic operations on tensors of different shapes without copying or expanding data manually.</p> <p>Imagine it as virtual expansion \u2014 PyTorch stretches the smaller tensor across the bigger one without allocating new memory.</p>"},{"location":"chapter8_broadcast_shape/#example","title":"Example:","text":"<p><pre><code>a = torch.tensor([[1], [2], [3]])   # Shape: (3, 1)\nb = torch.tensor([10, 20])         # Shape: (2,)\nc = a + b                          # Shape: (3, 2)\n</code></pre> Here\u2019s what PyTorch imagines behind the scenes: <pre><code>a = [[1],    b = [10, 20]   \u2192   [[1+10, 1+20],\n     [2],                      [2+10, 2+20],\n     [3]]                     [3+10, 3+20]]\n</code></pre></p> <p>No manual tiling. No sweat.</p>"},{"location":"chapter8_broadcast_shape/#82-broadcasting-rules","title":"8.2 Broadcasting Rules","text":"<p>To broadcast two tensors:</p> <ol> <li> <p>Start from the trailing dimensions (i.e., compare right to left).</p> </li> <li> <p>Dimensions must be:</p> <ul> <li> <p>Equal, OR</p> </li> <li> <p>One of them is 1, OR</p> </li> <li> <p>One is missing (implied 1)</p> </li> </ul> </li> </ol> <pre><code>Shape A     Shape B     Result Shape        Valid?\n\n(3, 1)      (1, 4)      (3, 4)              \u2705\n\n(2, 3)      (3,)        (2, 3)              \u2705\n\n(2, 3)      (3, 2)      \u274c                  \u274c\n</code></pre>"},{"location":"chapter8_broadcast_shape/#83-shape-ops-you-must-know","title":"8.3 Shape Ops You Must Know","text":"<p>These are the reshape tools every PyTorch practitioner must master.</p>"},{"location":"chapter8_broadcast_shape/#reshape-vs-view","title":"\ud83d\udd39 <code>reshape()</code> vs <code>view()</code>","text":"<pre><code>x = torch.arange(6)        # [0, 1, 2, 3, 4, 5]\nx.reshape(2, 3)            # OK anytime\nx.view(2, 3)               # Only if x is contiguous\n</code></pre> <p><code>reshape()</code> is <code>safer</code>, <code>view()</code> is faster but stricter.</p>"},{"location":"chapter8_broadcast_shape/#squeeze-and-unsqueeze","title":"\ud83d\udd39 <code>squeeze()</code> and <code>unsqueeze()</code>","text":"<ul> <li><code>squeeze()</code> removes dimensions of size 1</li> <li><code>nsqueeze(dim)</code> adds a 1-sized dimension at position <code>dim</code> <pre><code>x = torch.zeros(1, 3, 1)\nx.squeeze()       # shape: (3,)\nx.unsqueeze(0)    # shape: (1, 1, 3, 1)\n</code></pre> <p>Essential for converting between batch and single-item tensors.</p> </li> </ul>"},{"location":"chapter8_broadcast_shape/#expand-vs-repeat","title":"\ud83d\udd39 <code>expand()</code> vs <code>repeat()</code>","text":"<p>Both make a tensor appear larger \u2014 but in very different ways.  </p> <ul> <li><code>expand</code>(): No memory copy. Just a view. <pre><code>x = torch.tensor([[1], [2]])\nx.expand(2, 3)  # OK: repeats the column virtually\n</code></pre></li> <li><code>repeat()</code>: Physically copies data. <pre><code>x.repeat(1, 3)   # Actually allocates more memory\n</code></pre> <p>\u2705 Use expand() when possible. It\u2019s faster and leaner.</p> </li> </ul>"},{"location":"chapter8_broadcast_shape/#permute-and-transpose","title":"\ud83d\udd39 <code>permute()</code> and <code>transpose()</code>","text":"<ul> <li> <p>permute() \u2014 changes any dimension order <pre><code>x = torch.randn(2, 3, 4)\nx.permute(2, 0, 1)  # new shape: (4, 2, 3)\n</code></pre></p> </li> <li> <p><code>transpose(dim0, dim1)</code> \u2014 swaps two dimensions <pre><code>x.transpose(0, 1)\n</code></pre></p> <p>Use <code>permute()</code> for more complex reordering (e.g., images \u2192 channels-first/last).</p> </li> </ul>"},{"location":"chapter8_broadcast_shape/#84-real-world-use-cases","title":"8.4 Real-World Use Cases","text":"Task Operation Needed Convert grayscale to batch <code>unsqueeze(0)</code> Flatten a CNN layer output <code>.view(batch_size, -1)</code> Add channel dim to image <code>unsqueeze(0)</code> or <code>permute()</code> Match label shapes for loss <code>squeeze()</code> Expand bias term in matmul <code>expand()</code>"},{"location":"chapter8_broadcast_shape/#85-common-pitfalls","title":"8.5 Common Pitfalls","text":"<ul> <li> <p>Incompatible shapes: Use <code>.shape</code> to debug before applying ops.</p> </li> <li> <p><code>view()</code> on non-contiguous tensors: Use <code>.contiguous()</code> or switch to <code>reshape()</code>.</p> </li> <li> <p>Unintended broadcasting: Always print tensor shapes if math results look suspicious.</p> </li> </ul>"},{"location":"chapter8_broadcast_shape/#86-summary","title":"8.6 Summary","text":"<ul> <li> <p>Broadcasting enables operations on mismatched shapes.</p> </li> <li> <p>Reshape tools like <code>view</code>, <code>reshape</code>, <code>squeeze</code>, and <code>unsqueeze</code> give full control over dimensions.</p> </li> <li> <p><code>expand()</code> is fast and memory-efficient \u2014 use it over <code>repeat()</code> when possible.</p> </li> <li> <p>Shape ops are essential for building models, writing clean data pipelines, and debugging runtime errors.</p> </li> </ul>"},{"location":"chapter9_autograd/","title":"Chapter 9: Autograd and Differentiation","text":"<p>\u201cIf tensors are the muscles, autograd is the nervous system.\u201d</p>"},{"location":"chapter9_autograd/#91-what-is-autograd","title":"9.1 What Is Autograd?","text":"<p>Autograd is PyTorch\u2019s automatic differentiation engine. It builds a computation graph behind the scenes as you operate on tensors with <code>requires_grad=True</code>. When you call <code>.backward()</code>, it traces back through that graph to compute gradients.</p> <p>This is what powers training in PyTorch \u2014 from basic logistic regression to massive transformers.</p>"},{"location":"chapter9_autograd/#92-enabling-gradient-tracking","title":"9.2 Enabling Gradient Tracking","text":"<p>To start tracking gradients:</p> <pre><code>x = torch.tensor([2.0, 3.0], requires_grad=True)\n</code></pre> <p>Now, any operation on <code>x</code> will be recorded: <pre><code>y = x ** 2 + 3\nz = y.sum()\nz.backward()\nprint(x.grad)  # Output: tensor([4., 6.])\n</code></pre> Here, \u2202z/\u2202x = 2x \u2192 [2\u00d72, 2\u00d73] = [4., 6.]</p>"},{"location":"chapter9_autograd/#93-calling-backward","title":"9.3 Calling <code>.backward()</code>","text":"<p>Once you have a scalar result (like loss), call: <pre><code>loss = model(input).sum()\nloss.backward()\n</code></pre> PyTorch will:</p> <ul> <li> <p>Walk backward through the computation graph</p> </li> <li> <p>Calculate gradients for every tensor with <code>requires_grad=True</code></p> </li> <li> <p>Store gradients in the <code>.grad</code> attribute</p> </li> </ul>"},{"location":"chapter9_autograd/#94-stopping-gradient-tracking","title":"9.4 Stopping Gradient Tracking","text":"<p>When you want to freeze parts of the model (e.g., during evaluation or feature extraction), use:</p> <p>Method 1: with <code>torch.no_grad()</code> <pre><code>with torch.no_grad():\n    y = model(x)\n</code></pre> Method 2: <code>detach()</code> <pre><code>x = torch.tensor([1.0], requires_grad=True)\ny = x * 2\nz = y.detach()  # z does not track gradients\n</code></pre></p>"},{"location":"chapter9_autograd/#95-checking-the-computation-graph","title":"9.5 Checking the Computation Graph","text":"<p>You can inspect how PyTorch built the graph: <pre><code>x = torch.tensor([2.0], requires_grad=True)\ny = x * x\nprint(y.grad_fn)  # Output: &lt;MulBackward0&gt;\n</code></pre></p> <p>Every operation creates a <code>Function</code> object like <code>AddBackward0</code>, <code>MulBackward0</code>, etc. This is how PyTorch knows how to differentiate each step.</p>"},{"location":"chapter9_autograd/#96-torchautogradfunction-custom-gradients","title":"9.6 <code>torch.autograd.Function</code>: Custom Gradients","text":"<p>If you want to write your own forward/backward logic (like building a custom layer or operator): <pre><code>class Square(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input):\n        ctx.save_for_backward(input)\n        return input ** 2\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, = ctx.saved_tensors\n        return grad_output * 2 * input\n</code></pre></p> <p>Use it like: <pre><code>x = torch.tensor([3.0], requires_grad=True)\ny = Square.apply(x)\ny.backward()\nprint(x.grad)  # Should be 6.0\n</code></pre></p> <p>\ud83d\udccc Advanced, but useful for low-level ops and optimization research.</p>"},{"location":"chapter9_autograd/#97-common-mistakes","title":"9.7 Common Mistakes","text":"Mistake Fix Calling <code>.backward()</code> on non-scalar Pass a gradient argument: <code>z.backward(torch.ones_like(z))</code> Using <code>.data</code> instead of <code>.detach()</code> Use <code>.detach()</code> \u2014 <code>.data</code> is risky In-place ops corrupting graph Avoid <code>x += ...</code> with autograd-tied tensors Forgetting .<code>zero_()</code> on <code>.grad</code> Always zero gradients before <code>.backward()</code> <pre><code>optimizer.zero_grad()  # Or model.zero_grad()\nloss.backward()\noptimizer.step()\n</code></pre>"},{"location":"chapter9_autograd/#98-gradient-accumulation","title":"9.8 Gradient Accumulation","text":"<p>By default, gradients accumulate: <pre><code>x = torch.tensor([2.0], requires_grad=True)\ny = x * 2\ny.backward()\ny.backward()\nprint(x.grad)  # Will be 4 + 4 = 8\n</code></pre> Use x.grad.zero_() or optimizer.zero_grad() to prevent this.</p>"},{"location":"chapter9_autograd/#99-summary","title":"9.9 Summary","text":"<ul> <li> <p><code>requires_grad=True</code> enables gradient tracking for a tensor.</p> </li> <li> <p><code>.backward()</code> triggers backpropagation from a scalar output.</p> </li> <li> <p>Gradients are stored in <code>.grad</code>.</p> </li> <li> <p>Use <code>no_grad()</code> or <code>detach()</code> to stop tracking.</p> </li> <li> <p>Autograd builds a dynamic graph as you run \u2014 no static declarations.</p> </li> </ul>"}]}